{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10d89fb-78c7-4e87-9d0e-e63755d28783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "import whisper_timestamped as whisper\n",
    "\n",
    "from pytube import YouTube\n",
    "\n",
    "import whisper\n",
    "\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ee1267-f9ec-4d84-a00e-4714c3cc1d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path_youtube = \"YoutubeAudios\"\n",
    "output_path_transcription = \"transcriptions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f281692-2ad0-415b-aed3-84c8adf16085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 17:11:22,359 ./logs/auto-labeler INFO extract_main_domain_and_video_id [3598473496.py]\n",
      "2023-09-29 17:11:22,360 ./logs/auto-labeler INFO download_youtube [3598473496.py]\n",
      "2023-09-29 17:11:23,525 ./logs/auto-labeler INFO Audio downloaded to YoutubeAudios/youtube_UyoXmHS-KGc.mp3 [3598473496.py]\n",
      "2023-09-29 17:11:23,526 ./logs/auto-labeler INFO transcribe_audio [3598473496.py]\n",
      "/Users/saazizi/miniconda3/lib/python3.10/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "2023-09-29 17:11:41,418 ./logs/auto-labeler INFO merge_segments [3598473496.py]\n",
      "2023-09-29 17:11:41,420 ./logs/auto-labeler INFO write_to_json [3598473496.py]\n",
      "2023-09-29 17:11:41,424 ./logs/auto-labeler INFO Transcription downloaded to transcriptions/youtube_UyoXmHS-KGc.json [3598473496.py]\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "import json\n",
    "import whisper  # Assuming you have a package or module named whisper\n",
    "from logger import logger\n",
    "\n",
    "class YouTubeTranscriber:\n",
    "\n",
    "    def __init__(self, url, output_path_youtube, output_path_transcription):\n",
    "        self.output_path_youtube = output_path_youtube\n",
    "        self.yt = YouTube(url)\n",
    "        self.transcription = None\n",
    "        self.url = url\n",
    "        self.filename_path = None \n",
    "        self.output_path_transcription = output_path_transcription\n",
    "\n",
    "    def extract_main_domain_and_video_id(self):\n",
    "        parsed_url = urlparse(self.url)\n",
    "        domain_parts = parsed_url.netloc.split('.')\n",
    "        main_domain = domain_parts[-2] if len(domain_parts) >= 2 else None\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        video_id = query_params.get('v', [None])[0]\n",
    "        self.video_id = f\"{main_domain}_{video_id}\"\n",
    "\n",
    "    def download_youtube(self):\n",
    "        self.filename = f\"{self.video_id}.mp3\"\n",
    "        \n",
    "        audio_stream = self.yt.streams.filter(only_audio=True).first()\n",
    "        \n",
    "        audio_stream.download(output_path=self.output_path_youtube, filename=self.filename)\n",
    "        logger.info(f\"Audio downloaded to {self.output_path_youtube}/{self.filename}\")\n",
    "\n",
    "\n",
    "    def transcribe_audio(self, model_name, device):\n",
    "        audio = whisper.load_audio(f\"{self.output_path_youtube}/{self.filename}\")\n",
    "        model = whisper.load_model(model_name, device=device)\n",
    "        self.transcription = whisper.transcribe(model, audio)\n",
    "\n",
    "    def write_to_json(self):\n",
    "        with open(f\"{self.output_path_youtube}/{self.video_id}.json\", 'w') as f:\n",
    "            json.dump(self.transcription, f)\n",
    "        logger.info(f\"Transcription downloaded to {self.output_path_transcription}/{self.video_id}.json\")\n",
    "\n",
    "    def merge_segments(self, num_to_merge):\n",
    "        merged_segments = []\n",
    "        segments = self.transcription[\"segments\"]\n",
    "        for i in range(0, len(segments), num_to_merge):\n",
    "            merged_dict = {}\n",
    "            slice_ = segments[i:i + num_to_merge]\n",
    "\n",
    "            # Merging the 'text' fields\n",
    "            merged_dict['text'] = \" \".join(item['text'] for item in slice_)\n",
    "\n",
    "            # Get the 'start' time from the first dictionary and the 'end' time from the last dictionary\n",
    "            merged_dict['start'] = int(slice_[0]['start'])\n",
    "            merged_dict['end'] = int(slice_[-1]['end'])\n",
    "\n",
    "  \n",
    "\n",
    "            merged_segments.append(merged_dict)\n",
    "\n",
    "        self.transcription[\"merged_segments\"] = merged_segments\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self, num_to_merge=4, model_name=\"base\", device=\"cpu\"):\n",
    "        \n",
    "        logger.info(\"extract_main_domain_and_video_id\")\n",
    "        self.extract_main_domain_and_video_id()\n",
    "        \n",
    "        logger.info(\"download_youtube\")\n",
    "        self.download_youtube()\n",
    "        \n",
    "        logger.info(\"transcribe_audio\")\n",
    "        self.transcribe_audio(model_name=model_name,\n",
    "                             device=device)\n",
    "        \n",
    "        logger.info(\"merge_segments\")\n",
    "        self.merge_segments(num_to_merge)\n",
    "        \n",
    "        logger.info(\"write_to_json\")\n",
    "        self.write_to_json()\n",
    "        \n",
    "\n",
    "\n",
    "# Usage\n",
    "output_path = output_path_youtube\n",
    "url = 'https://www.youtube.com/watch?v=5p248yoa3oE'\n",
    "url = \"https://www.youtube.com/watch?v=UyoXmHS-KGc\"\n",
    "yt_transcriber = YouTubeTranscriber(url=url, \n",
    "                                    output_path_youtube=output_path_youtube,\n",
    "                                   output_path_transcription=output_path_transcription)\n",
    "\n",
    "yt_transcriber.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8428855-52d9-46f3-823d-a6c1635012bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3572f0ed-2c8a-4a08-bf66-d1709319a8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the JSON file\n",
    "file_path = \"YoutubeAudios/002youtube_UyoXmHS-KGc.json\"\n",
    "\n",
    "# Open the file for reading\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b82884-09a5-4063-812d-6abc33a4026f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e064d49e-a5f8-4056-9fba-45ba87a7a0e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "356726ed-4e4f-4432-ac9c-76c843e8707d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6f1d6f7-4ab6-4d1d-93d3-e652ecb59b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a607983-184a-4a60-b77f-dbb35c86e4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "440f0aa9-9518-468d-b517-0bcd42387967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84c88bcd-0fd9-47e2-bf5d-424369f70d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "86d7cb06-1f3c-40a0-8dba-b044bb9c9a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21755f6b-21c7-4dd3-b30b-a10508b99f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c22233a8-748f-40d9-b915-ec54299c847e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: You're the best summarizer of video taks, either wabinars, talk, coatching, you can find the most relevent informations \n",
      "from the transcription of a video talk: \n",
      "\n",
      " Today we are going to look at an amazing new AI system that can perform 4 tricks. The first is cool, the second is great, the third one is simply incredible to the point that I couldn't believe the results and had to look over and over again. And the fourth is a thing of beauty. So what are the tricks? First, this work is called Gigagen and it can perform tax-to-image. We fellow scholars have seen this before many, many times you enter a tax prompt and it paints you an image. What is great about it is that it can give us reasonably high quality images, that is okay, but here is the kicker. It can perform this in a fraction of a second. That is extremely quick. For instance, the previous StarGain-based method that could be roughly as fast we needed to make significant concessions in terms of quality. Not anymore. Loving it. Two, it is not only fast, but it is so fast that it can create several images per second and thus it offers a controllable latent space. This is a hallmark for Gigagen-based methods and leads to incredible artistic controllability as you see here. But what does all this mean? Gigagen-based method means a generative adversarial network where two neural networks battle each other. One tries to generate new images to fool the other one while the other one trains to be able to spot the synthetic images. Over time, they battle each other and they improve together. Now third and this is where things get out of hand. Super resolution or in other words, image upscaling. Here a course image goes in and the AI guesses what this image could be and synthesizes a new really detailed image. Now hold on to your papers and look at that. The difference can be really huge up to a thousand times more pixels in the new image. So how does it compare to previous methods? For instance, the amazing stable diffusion. Oh my, look at that. It is so much better across the board, pretty much everywhere. But the eyes, the eyes are truly something else with the new technique. What a time to be alive and finally fourth. It also offers a decent tangled latent space for controllability. Now I hear you asking, Doctor, what does that mean? This means that we can control the style in place with the text prompts. So whenever we get a teddy bear that we really like, but we would like to see it crocheted a bit fluffier or made of denim we can do that, but without generating a new teddy bear. All the changes are applied to the same subject. That is incredible. I also loved the ball example here. This almost looks like an image straight out of a material modeling paper in computer graphics. So cool. And what I absolutely love about this paper is that normally for these four problems we would need four separate tools. Not anymore. Today we are getting all of these amazing capabilities with just one tool that can do all of them and not only that, but it is so much faster than previous methods while it is still competitive in terms of visual quality. It is not the best and fastest at the same time, not even close. But this trade-off is, I think, an amazing value proposition. If you enjoyed this paper, make sure to subscribe and hit the bell icon to not miss out. We have some more amazing papers coming up soon. This video has been supported by weights and biases. Look at this. They have a great community forum that aims to make you the best machine learning engineer you can be. You see, I always get messages from you fellow scholars telling me that you have been inspired by the series, but don't really know where to start. And here it is. In this forum, you can share your projects, ask for advice, look for collaborators and more. Make sure to visit wnb.me slash paper forum and say hi or just click the link in the video description. Our thanks to weights and biases for their long standing support and for helping us make better videos for you. Thanks for watching and for your generous support and I'll see you next time.\n",
      "\n",
      "\n",
      "your response: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The speaker discusses a new AI system capable of performing four tricks, including tax-to-image generation, super resolution, and controllable latent space. The system uses a generative adversarial network (GAN) and differs from previous methods as it can generate high-quality images rapidly and produce multiple images per second. Additionally, the method provides a controllable latent space, allowing for artistic control over generated images. The speaker considers this technology a game-changer as it offers immense value by providing four capabilities with only one tool while being faster than previous techniques. The speaker encourages'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Replicate\n",
    "import openai\n",
    "template = \"\"\"You're the best summarizer of video taks, either wabinars, talk, coatching, you can find the most relevent informations \n",
    "from the transcription of a video talk: \n",
    "\n",
    "{transcription}\n",
    "\n",
    "\n",
    "your response: \"\"\"\n",
    "\n",
    "# Define the human message prompt template.\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"transcription\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the chat prompt template.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "\n",
    "llm = Replicate(\n",
    "            model=\"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "            model_kwargs={\"temperature\": 0.9, \"max_length\": 4000, \"top_p\": 1},\n",
    "        )\n",
    "chat = LLMChain(\n",
    "        llm=llm, prompt=chat_prompt_template, verbose=True\n",
    "    )\n",
    "chat.run(\n",
    "            {\"transcription\": data['text']}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21f9103e-23e2-4dd7-8926-35b5db0fd187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb8da33f-752f-4d37-86b9-b558a720bd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80fe70-0d40-463a-b733-2a0e5bdae710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64b50d-51ce-4179-aa34-ec711e3c8ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer",
   "language": "python",
   "name": "summarizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
