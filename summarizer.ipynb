{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6582057f-1135-4eec-afbb-793a4f61a63e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nb_black in /Users/saazizi/miniconda3/lib/python3.10/site-packages (1.0.7)\n",
      "Requirement already satisfied: black>='19.3' in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from nb_black) (23.7.0)\n",
      "Requirement already satisfied: ipython in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from nb_black) (8.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from black>='19.3'->nb_black) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from black>='19.3'->nb_black) (0.11.2)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from black>='19.3'->nb_black) (2.0.1)\n",
      "Requirement already satisfied: packaging>=22.0 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from black>='19.3'->nb_black) (23.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from black>='19.3'->nb_black) (8.1.3)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from black>='19.3'->nb_black) (3.10.0)\n",
      "Requirement already satisfied: decorator in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (4.4.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (0.1.6)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (3.0.36)\n",
      "Requirement already satisfied: stack-data in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (2.15.1)\n",
      "Requirement already satisfied: pickleshare in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (0.18.1)\n",
      "Requirement already satisfied: appnope in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (0.1.2)\n",
      "Requirement already satisfied: backcall in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from ipython->nb_black) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from jedi>=0.16->ipython->nb_black) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from pexpect>4.3->ipython->nb_black) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->nb_black) (0.2.5)\n",
      "Requirement already satisfied: asttokens in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from stack-data->ipython->nb_black) (2.0.5)\n",
      "Requirement already satisfied: executing in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from stack-data->ipython->nb_black) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from stack-data->ipython->nb_black) (0.2.2)\n",
      "Requirement already satisfied: six in /Users/saazizi/miniconda3/lib/python3.10/site-packages (from asttokens->stack-data->ipython->nb_black) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nb_black nbdime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584d7cc0-b77b-473a-821c-8145a49c6723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c10d89fb-78c7-4e87-9d0e-e63755d28783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "import whisper_timestamped as whisper\n",
    "from pydub import AudioSegment\n",
    "from logger import logger\n",
    "from pytube import YouTube\n",
    "\n",
    "import whisper\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "19760f48-d807-4870-bb67-3ebf87b4140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/openai/whisper-large-v2#long-form-transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "32ee1267-f9ec-4d84-a00e-4714c3cc1d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path_youtube = \"YoutubeAudios\"\n",
    "output_path_transcription = \"transcriptions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3f281692-2ad0-415b-aed3-84c8adf16085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 16:21:35,757 ./logs/auto-labeler INFO extract_main_domain_and_video_id [4038077557.py]\n",
      "2023-10-02 16:21:35,758 ./logs/auto-labeler INFO download_youtube [4038077557.py]\n",
      "2023-10-02 16:21:39,820 ./logs/auto-labeler INFO Audio downloaded to YoutubeAudios/youtube_5p248yoa3oE.mp3 [4038077557.py]\n",
      "2023-10-02 16:21:39,821 ./logs/auto-labeler INFO transcribe_audio [4038077557.py]\n",
      "/Users/saazizi/miniconda3/lib/python3.10/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "2023-10-02 16:24:00,239 ./logs/auto-labeler INFO merge_segments [4038077557.py]\n",
      "2023-10-02 16:24:00,241 ./logs/auto-labeler INFO write_to_json [4038077557.py]\n",
      "2023-10-02 16:24:00,249 ./logs/auto-labeler INFO Transcription downloaded to transcriptions/youtube_5p248yoa3oE.json [4038077557.py]\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "import json\n",
    "import whisper  # Assuming you have a package or module named whisper\n",
    "from logger import logger\n",
    "\n",
    "class YouTubeTranscriber:\n",
    "\n",
    "    def __init__(self, url, output_path_youtube, output_path_transcription):\n",
    "        self.output_path_youtube = output_path_youtube\n",
    "        self.yt = YouTube(url)\n",
    "        self.transcription = None\n",
    "        self.url = url\n",
    "        self.filename_path = None \n",
    "        self.output_path_transcription = output_path_transcription\n",
    "\n",
    "    def extract_main_domain_and_video_id(self):\n",
    "        parsed_url = urlparse(self.url)\n",
    "        domain_parts = parsed_url.netloc.split('.')\n",
    "        main_domain = domain_parts[-2] if len(domain_parts) >= 2 else None\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        video_id = query_params.get('v', [None])[0]\n",
    "        self.video_id = f\"{main_domain}_{video_id}\"\n",
    "\n",
    "    def download_youtube(self):\n",
    "        self.filename = f\"{self.video_id}.mp3\"\n",
    "        \n",
    "        audio_stream = self.yt.streams.filter(only_audio=True).first()\n",
    "        \n",
    "        audio_stream.download(output_path=self.output_path_youtube, filename=self.filename)\n",
    "        logger.info(f\"Audio downloaded to {self.output_path_youtube}/{self.filename}\")\n",
    "\n",
    "\n",
    "    def transcribe_audio(self, model_name, device):\n",
    "        audio = whisper.load_audio(f\"{self.output_path_youtube}/{self.filename}\")\n",
    "        model = whisper.load_model(model_name, device=device)\n",
    "        self.transcription = whisper.transcribe(model, audio)\n",
    "\n",
    "    def write_to_json(self):\n",
    "        with open(f\"{self.output_path_transcription}/{self.video_id}.json\", 'w') as f:\n",
    "            json.dump(self.transcription, f)\n",
    "        logger.info(f\"Transcription downloaded to {self.output_path_transcription}/{self.video_id}.json\")\n",
    "\n",
    "    def merge_segments(self, num_to_merge):\n",
    "        merged_segments = []\n",
    "        segments = self.transcription[\"segments\"]\n",
    "        for i in range(0, len(segments), num_to_merge):\n",
    "            merged_dict = {}\n",
    "            slice_ = segments[i:i + num_to_merge]\n",
    "\n",
    "            # Merging the 'text' fields\n",
    "            merged_dict['text'] = \" \".join(item['text'] for item in slice_)\n",
    "\n",
    "            # Get the 'start' time from the first dictionary and the 'end' time from the last dictionary\n",
    "            merged_dict['start'] = int(slice_[0]['start'])\n",
    "            merged_dict['end'] = int(slice_[-1]['end'])\n",
    "\n",
    "  \n",
    "\n",
    "            merged_segments.append(merged_dict)\n",
    "\n",
    "        self.transcription[\"merged_segments\"] = merged_segments\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self, num_to_merge=4, model_name=\"base\", device=\"cpu\"):\n",
    "        \n",
    "        logger.info(\"extract_main_domain_and_video_id\")\n",
    "        self.extract_main_domain_and_video_id()\n",
    "        \n",
    "        logger.info(\"download_youtube\")\n",
    "        self.download_youtube()\n",
    "        \n",
    "        logger.info(\"transcribe_audio\")\n",
    "        self.transcribe_audio(model_name=model_name,\n",
    "                             device=device)\n",
    "        \n",
    "        logger.info(\"merge_segments\")\n",
    "        self.merge_segments(num_to_merge)\n",
    "        \n",
    "        logger.info(\"write_to_json\")\n",
    "        self.write_to_json()\n",
    "        \n",
    "\n",
    "\n",
    "# Usage\n",
    "output_path = output_path_youtube\n",
    "url = 'https://www.youtube.com/watch?v=5p248yoa3oE'\n",
    "# url = \"https://www.youtube.com/watch?v=UyoXmHS-KGc\"\n",
    "yt_transcriber = YouTubeTranscriber(url=url, \n",
    "                                    output_path_youtube=output_path_youtube,\n",
    "                                   output_path_transcription=output_path_transcription)\n",
    "\n",
    "yt_transcriber.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4268b1-456d-4e93-90eb-53b0d2e56850",
   "metadata": {},
   "source": [
    "# LamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4cebcec-23a6-4fae-a3b3-290779e0da47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "import json\n",
    "from logger import logger\n",
    "from llama_index.llms import OpenAI\n",
    "# from service_context.node_parser import NodeParser\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6ecccbb-632b-4502-95ae-d7b8472382ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "# service_context = ServiceContext.from_defaults(chunk_size=chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e0fff7b-5032-4f73-be11-b5b3bfe82e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6525762-53a2-47ac-8fa0-d332d7f2e3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c29650c1-6549-42b0-8dcc-738d02d0120f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_nodes_from_transcription(json_file, chunk_sizes):\n",
    "    \"\"\"\n",
    "    Concatenates words from a given JSON data structure with maximum chunk sizes.\n",
    "\n",
    "    Parameters:\n",
    "        json_file (str): The path to the JSON file.\n",
    "        chunk_sizes (list of int): The list of maximum number of words allowed in each concatenated chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of nodes. Each node contains concatenated text and metadata (start_time, end_time).\n",
    "\n",
    "    \"\"\"\n",
    "    def add_document_to_chunks(word_list, start_time, end_time, chunks,chunk_size):\n",
    "        \"\"\"Helper function to add a new Document to chunks.\"\"\"\n",
    "        text = \" \".join(word_list)\n",
    "        doc = Document(text=text, extra_info={'start': start_time, \n",
    "                                              \"end\": end_time,\n",
    "                                             \"chunk_size\":chunk_size})\n",
    "        chunks.append(doc)\n",
    "\n",
    "    concatenated_chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "    current_start_time = None\n",
    "    current_end_time = None\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(json_file, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Initialize service context\n",
    "    llm = OpenAI(model=\"gpt-4\")\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    \n",
    "    if not isinstance(chunk_sizes, list):\n",
    "        chunk_sizes = [chunk_sizes]\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for segment in json_data[\"segments\"]:\n",
    "            for word_info in segment[\"words\"]:\n",
    "                if current_word_count + 1 > chunk_size:\n",
    "                    add_document_to_chunks(current_chunk, current_start_time, current_end_time, concatenated_chunks, chunk_size)\n",
    "                    current_chunk = []\n",
    "                    current_word_count = 0\n",
    "                    current_start_time = None\n",
    "\n",
    "                current_chunk.append(word_info[\"word\"].strip())\n",
    "                current_word_count += 1\n",
    "\n",
    "                if current_start_time is None:\n",
    "                    current_start_time = word_info[\"start\"]\n",
    "                current_end_time = word_info[\"end\"]\n",
    "\n",
    "        if current_chunk:\n",
    "            add_document_to_chunks(current_chunk, current_start_time, current_end_time, concatenated_chunks, chunk_size)\n",
    "\n",
    "        nodes = service_context.node_parser.get_nodes_from_documents(concatenated_chunks)\n",
    "        \n",
    "        # for node in nodes:\n",
    "        #     node.metadata[\"chunk_size\"] = chunk_size\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "# Example usage\n",
    "chunk_sizes = [128, 256, 512, 1024]\n",
    "nodes = get_nodes_from_transcription(json_file=\"transcriptions/youtube_UyoXmHS-KGc.json\", chunk_sizes=chunk_sizes)\n",
    "len(nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1f37832-ebbf-45b5-8baa-14eb86cde980",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 15:36:41,421 ./logs/auto-labeler INFO add chunk size to nodes to track later [1093469001.py]\n",
      "2023-10-02 15:36:41,422 ./logs/auto-labeler INFO build vector index [1093469001.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff53253c5d404c2ba3066b4273f6acf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 15:36:42,403 ./logs/auto-labeler INFO query engines [1093469001.py]\n",
      "2023-10-02 15:36:42,442 ./logs/auto-labeler INFO add chunk size to nodes to track later [1093469001.py]\n",
      "2023-10-02 15:36:42,443 ./logs/auto-labeler INFO build vector index [1093469001.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cb1e5ac8fc4ce28c431c70004c20f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 15:36:43,025 ./logs/auto-labeler INFO query engines [1093469001.py]\n",
      "2023-10-02 15:36:43,038 ./logs/auto-labeler INFO add chunk size to nodes to track later [1093469001.py]\n",
      "2023-10-02 15:36:43,038 ./logs/auto-labeler INFO build vector index [1093469001.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779ee83795f7411eacd25ed3a7dde291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 15:36:43,581 ./logs/auto-labeler INFO query engines [1093469001.py]\n",
      "2023-10-02 15:36:43,591 ./logs/auto-labeler INFO add chunk size to nodes to track later [1093469001.py]\n",
      "2023-10-02 15:36:43,592 ./logs/auto-labeler INFO build vector index [1093469001.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 1024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d477ed9a36a14aa19b23bf31fe2703db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 15:36:44,167 ./logs/auto-labeler INFO query engines [1093469001.py]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "nodes_list = []\n",
    "vector_indices = []\n",
    "query_engines = []\n",
    "chunk_sizes = [128, 256, 512, 1024]\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"Chunk Size: {chunk_size}\")\n",
    "    nodes = get_nodes_from_transcription(json_file=\"transcriptions/youtube_UyoXmHS-KGc.json\", chunk_sizes=chunk_size)\n",
    "    \n",
    "\n",
    "    # add chunk size to nodes to track later\n",
    "    logger.info(\"add chunk size to nodes to track later\")\n",
    "    for node in nodes:\n",
    "        node.metadata[\"chunk_size\"] = chunk_size\n",
    "        node.excluded_embed_metadata_keys = [\"chunk_size\"]\n",
    "        node.excluded_llm_metadata_keys = [\"chunk_size\"]\n",
    "\n",
    "    nodes_list.append(nodes)\n",
    "\n",
    "    # build vector index\n",
    "    logger.info(\"build vector index\")\n",
    "    vector_index = VectorStoreIndex(nodes, show_progress=True)\n",
    "    vector_indices.append(vector_index)\n",
    "\n",
    "    # query engines\n",
    "    logger.info(\"query engines\")\n",
    "    \n",
    "    query_engines.append(vector_index.as_query_engine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6e6f1ff-2049-4b6f-8c02-45f5f0e8a870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = query_engines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "188d68c9-1281-46db-8e51-5f7b048b450d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='You can find advice in the forum mentioned in the context.', source_nodes=[NodeWithScore(node=TextNode(id_='9d7b4452-0201-48c1-92ef-4e8890141e0a', embedding=None, metadata={'start': 258.08, 'end': 293.92, 'chunk_size': 128}, excluded_embed_metadata_keys=['chunk_size'], excluded_llm_metadata_keys=['chunk_size'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4a50f545-8f52-458a-a040-20f482cd9f77', node_type=None, metadata={'start': 258.08, 'end': 293.92, 'chunk_size': 128}, hash='20cc92ce5293b915b7c6f133ececfa84401db5f8cafd638b3b669b98277d0732')}, hash='20cc92ce5293b915b7c6f133ececfa84401db5f8cafd638b3b669b98277d0732', text=\"from you fellow scholars telling me that you have been inspired by the series, but don't really know where to start. And here it is. In this forum, you can share your projects, ask for advice, look for collaborators, and more. Make sure to visit www .me -slash -paper -forum and say hi or just click the link in the video description. Our thanks to weights and biases for their long -standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7826773023070115), NodeWithScore(node=TextNode(id_='b46cfbc4-d968-441a-9302-bf385d788668', embedding=None, metadata={'start': 211.56, 'end': 258.08, 'chunk_size': 128}, excluded_embed_metadata_keys=['chunk_size'], excluded_llm_metadata_keys=['chunk_size'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a3121f24-e463-4db0-a75a-d7f9032d2735', node_type=None, metadata={'start': 211.56, 'end': 258.08, 'chunk_size': 128}, hash='127a8f5147faf62ca4568f52805995cfdf1b6e991ba80c4f85dee98cb90a01aa')}, hash='127a8f5147faf62ca4568f52805995cfdf1b6e991ba80c4f85dee98cb90a01aa', text='getting all of these amazing capabilities with just one tool that can do all of them. And not only that, but it is so much faster than previous methods, while it is still competitive in terms of visual quality. It is not the best and fastest at the same time, not even close. But this trade -off is, I think, an amazing value proposition. If you enjoyed this paper, make sure to subscribe and hit the bell icon to not miss out. We have some more amazing papers coming up soon. This video has been supported by weights and biases. Look at this. They have a great community forum that aims to make you the best machine learning engineer you can be. You see, I always get messages', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7461206379266797)], metadata={'9d7b4452-0201-48c1-92ef-4e8890141e0a': {'start': 258.08, 'end': 293.92, 'chunk_size': 128}, 'b46cfbc4-d968-441a-9302-bf385d788668': {'start': 211.56, 'end': 258.08, 'chunk_size': 128}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.query('advice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0b4f44c5-97cb-4732-9a2e-87af4f9f6522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2ff136a-4e92-47e7-ad1b-25af5d52dbea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_128\n",
      "chunk_256\n",
      "chunk_512\n",
      "chunk_1024\n"
     ]
    }
   ],
   "source": [
    "# try ensemble retrieval\n",
    "\n",
    "from llama_index.tools import RetrieverTool\n",
    "from llama_index.schema import IndexNode\n",
    "\n",
    "# retriever_tools = []\n",
    "retriever_dict = {}\n",
    "retriever_nodes = []\n",
    "for chunk_size, vector_index in zip(chunk_sizes, vector_indices):\n",
    "    node_id = f\"chunk_{chunk_size}\"\n",
    "    print(node_id)\n",
    "    node = IndexNode(\n",
    "        text=f\"Retrieves relevant advice (chunk size {chunk_size})\",\n",
    "        index_id=node_id,\n",
    "    )\n",
    "    retriever_nodes.append(node)\n",
    "    retriever_dict[node_id] = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a0bded1-ef0e-4541-b9d0-dc0f8b7f77ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = retriever_dict['chunk_128']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cf2adf3-be02-4da8-b6a1-850039bf1de6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='7fc0010f-3c17-4278-a13e-a5486df563dc', embedding=None, metadata={'start': 49.02, 'end': 102.68, 'chunk_size': 128}, excluded_embed_metadata_keys=['chunk_size'], excluded_llm_metadata_keys=['chunk_size'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='35b68b88-fe29-42d6-916c-689b651c724d', node_type=None, metadata={'start': 49.02, 'end': 102.68, 'chunk_size': 128}, hash='f771b1137f9784b6d21604e6698d65ab116b20050f687025185e3238423d4248')}, hash='f771b1137f9784b6d21604e6698d65ab116b20050f687025185e3238423d4248', text='That is extremely quick. For instance, the previous StarGain -based method that could be roughly as fast we needed to make significant concessions in terms of quality. Not anymore. Loving it. Two, it is not only fast, but it is so fast that it can create several images per second and thus it offers the controllable latent space. This is a hallmark for Gigagen -based methods and leads to incredible artistic controllability as you see here. But what does all this mean? Gigagen -based method means a generative adversarial network where two neural networks battle each other. One tries to generate new images to fool the other one while the other one trains to be able to spot the synthetic images. Over time, they battle each other and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8571730377381808),\n",
       " NodeWithScore(node=TextNode(id_='b8570026-9657-45e8-b6b6-337be042928d', embedding=None, metadata={'start': 0.0, 'end': 48.64, 'chunk_size': 128}, excluded_embed_metadata_keys=['chunk_size'], excluded_llm_metadata_keys=['chunk_size'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fcf34d0c-6c49-43af-a386-419503c0ecde', node_type=None, metadata={'start': 0.0, 'end': 48.64, 'chunk_size': 128}, hash='6138c94d81ba37b62ad939907a115fb34a3da53890788ae519212eabd2c9b674')}, hash='6138c94d81ba37b62ad939907a115fb34a3da53890788ae519212eabd2c9b674', text=\"Today we are going to look at an amazing new AI system that can perform 4 tricks. The first is cool, the second is great, the third one is simply incredible to the point that I couldn't believe the results and had to look over and over again. And the fourth is a thing of beauty. So what are the tricks? First, this work is called Gigagen and it can perform tax -to -image. We fellow scholars have seen this before many, many times you enter a tax prompt and it paints you an image. What is great about it is that it can give us reasonably high quality images, that is okay, but here is the kicker. It can perform this in a fraction of a second.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8327866395473805)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.retrieve(\"Gigagen-based method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3966c21-75c8-4a19-9366-d3c37b6b600c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.selectors.pydantic_selectors import PydanticMultiSelector\n",
    "\n",
    "# from llama_index.retrievers import RouterRetriever\n",
    "from llama_index.retrievers import RecursiveRetriever\n",
    "from llama_index import SummaryIndex\n",
    "\n",
    "# the derived retriever will just retrieve all nodes\n",
    "summary_index = SummaryIndex(retriever_nodes)\n",
    "\n",
    "retriever = RecursiveRetriever(\n",
    "    root_id=\"root\",\n",
    "    retriever_dict={\"root\": summary_index.as_retriever(), **retriever_dict},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ace59be0-bdf5-4890-a114-c93ace54922f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await retriever.aretrieve(\n",
    "    \"Gigagen\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee9963aa-3eeb-4c0e-81a6-ade45e28f999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement service-context (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for service-context\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install service-context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ed2f72e-d56a-4ce5-9a9d-99d299f14493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RecursiveRetriever.retrieve_all() missing 1 required positional argument: 'query_bundle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: RecursiveRetriever.retrieve_all() missing 1 required positional argument: 'query_bundle'"
     ]
    }
   ],
   "source": [
    "retriever.retrieve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7930ce9a-1d81-465d-a0ba-49756d6d80ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define reranker\n",
    "from llama_index.indices.postprocessor import (\n",
    "    LLMRerank,\n",
    "    SentenceTransformerRerank,\n",
    "    CohereRerank,\n",
    ")\n",
    "\n",
    "reranker = LLMRerank()\n",
    "# reranker = SentenceTransformerRerank(top_n=10)\n",
    "\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever, node_postprocessors=[reranker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7023418a-5fa8-45a7-9480-abc24a0c065c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x28c6274f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "229b4503-7912-4116-b171-9b4c2ab0baa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"Gigagen methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d62a4-4583-4c6b-bb40-20b3d244986e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8c7f9951-60d7-45a8-80e7-37c43800774a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.get_service_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc6a33-6002-4e85-a7c3-ca9ca53ef3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e064d49e-a5f8-4056-9fba-45ba87a7a0e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8ee83-3481-4118-b31a-c4ba951f7689",
   "metadata": {},
   "source": [
    "## retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1219f7df-533a-4f5a-8607-caa2be560a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3d4016c1-90da-4667-bf39-4fc5bb346964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_documents_from_transcription(json_file, chunk_sizes):\n",
    "    \"\"\"\n",
    "    Concatenates words from a given JSON data structure with maximum chunk sizes.\n",
    "\n",
    "    Parameters:\n",
    "        json_file (str): The path to the JSON file.\n",
    "        chunk_sizes (list of int): The list of maximum number of words allowed in each concatenated chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of nodes. Each node contains concatenated text and metadata (start_time, end_time).\n",
    "\n",
    "    \"\"\"\n",
    "    def add_document_to_chunks(word_list, start_time, end_time, chunks,chunk_size):\n",
    "        \"\"\"Helper function to add a new Document to chunks.\"\"\"\n",
    "        text = \" \".join(word_list)\n",
    "        doc = Document(page_content=text, metadata={'start': start_time, \n",
    "                                              \"end\": end_time,\n",
    "                                             \"chunk_size\":chunk_size})\n",
    "        chunks.append(doc)\n",
    "\n",
    "    concatenated_chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "    current_start_time = None\n",
    "    current_end_time = None\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(json_file, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Initialize service context\n",
    "    llm = OpenAI(model=\"gpt-4\")\n",
    "    service_context = ServiceContext.from_defaults(llm=llm)\n",
    "    \n",
    "    if not isinstance(chunk_sizes, list):\n",
    "        chunk_sizes = [chunk_sizes]\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for segment in json_data[\"segments\"]:\n",
    "            for word_info in segment[\"words\"]:\n",
    "                if current_word_count + 1 > chunk_size:\n",
    "                    add_document_to_chunks(current_chunk, current_start_time, current_end_time, concatenated_chunks, chunk_size)\n",
    "                    current_chunk = []\n",
    "                    current_word_count = 0\n",
    "                    current_start_time = None\n",
    "\n",
    "                current_chunk.append(word_info[\"word\"].strip())\n",
    "                current_word_count += 1\n",
    "\n",
    "                if current_start_time is None:\n",
    "                    current_start_time = word_info[\"start\"]\n",
    "                current_end_time = word_info[\"end\"]\n",
    "\n",
    "        if current_chunk:\n",
    "            add_document_to_chunks(current_chunk, current_start_time, current_end_time, concatenated_chunks, chunk_size)\n",
    "\n",
    "    return concatenated_chunks\n",
    "\n",
    "\n",
    "# Example usage\n",
    "chunk_sizes = [128, 256, 512, 1024]\n",
    "docs = get_documents_from_transcription(json_file=\"transcriptions/youtube_UyoXmHS-KGc.json\", chunk_sizes=chunk_sizes)\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a8c88f2-f4b1-4acb-80e3-549b4e55b4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "19367e9d-9d11-476d-bc95-5e19e3622d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load blog post\n",
    "# loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "# data = loader.load()\n",
    "\n",
    "# # Split\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "# splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d9f6721e-89bd-483a-bc7c-a70eef63bed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiQueryRetriever(tags=None, metadata=None, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x17f62ac20>, search_type='similarity', search_kwargs={}), llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=LineListOutputParser(pydantic_object=<class 'langchain.retrievers.multi_query.LineList'>), return_final_only=True, llm_kwargs={}), verbose=True, parser_key='lines')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = \"advice\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")\n",
    "retriever_from_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43806caa-8352-4002-8a29-3b08708d1019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"from you fellow scholars telling me that you have been inspired by the series, but don't really know where to start. And here it is. In this forum, you can share your projects, ask for advice, look for collaborators, and more. Make sure to visit www .me -slash -paper -forum and say hi or just click the link in the video description. Our thanks to weights and biases for their long -standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time.\", metadata={'chunk_size': 128, 'end': 293.92, 'start': 258.08}),\n",
       " Document(page_content=\"projects, ask for advice, look for collaborators, and more. Make sure to visit www .me -slash -paper -forum and say hi or just click the link in the video description. Our thanks to weights and biases for their long -standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time.\", metadata={'chunk_size': 256, 'end': 293.92, 'start': 269.22}),\n",
       " Document(page_content=\"from you fellow scholars telling me that you have been inspired by the series, but don't really know where to start. And here it is. In this forum, you can share your projects, ask for advice, look for collaborators, and more. Make sure to visit www .me -slash -paper -forum and say hi or just click the link in the video description. Our thanks to weights and biases for their long -standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time. Today we are going to look at an amazing new AI system that can perform 4 tricks. The first is cool, the second is great, the third one is simply incredible to the point that I couldn't believe the results and had to look over and over again. And the fourth is a thing of beauty. So what are the tricks? First, this work is called Gigagen and it can perform tax -to -image. We fellow scholars have seen this before many, many times you enter a tax prompt and it paints you an image. What is great about it is that it can give us reasonably high quality images, that is okay, but here is the kicker. It can perform this in a fraction of a second. That is extremely quick. For instance, the previous StarGain -based method that could be roughly as fast we needed to make significant concessions in terms of quality. Not anymore. Loving it. Two,\", metadata={'chunk_size': 256, 'end': 63.34, 'start': 258.08}),\n",
       " Document(page_content=\"bear. All the changes are applied to the same subject. That is incredible. I also loved the ball example here. This almost looks like an image straight out of a material modeling paper in computer graphics. So cool! And what I absolutely love about this paper is that normally, for these four problems, we would need four separate tools. Not anymore. Today, we are getting all of these amazing capabilities with just one tool that can do all of them. And not only that, but it is so much faster than previous methods, while it is still competitive in terms of visual quality. It is not the best and fastest at the same time, not even close. But this trade -off is, I think, an amazing value proposition. If you enjoyed this paper, make sure to subscribe and hit the bell icon to not miss out. We have some more amazing papers coming up soon. This video has been supported by weights and biases. Look at this. They have a great community forum that aims to make you the best machine learning engineer you can be. You see, I always get messages from you fellow scholars telling me that you have been inspired by the series, but don't really know where to start. And here it is. In this forum, you can share your projects, ask for advice, look for collaborators, and more. Make sure to visit www .me -slash -paper -forum and say hi or just click the link in the video description. Our thanks to weights and biases for their long -standing support and for helping us make better videos for you. Thanks for watching and for your generous support, and I'll see you next time.\", metadata={'chunk_size': 512, 'end': 293.92, 'start': 185.22}),\n",
       " Document(page_content='they improve together. Now, third and this is where things get out of hand, super -resolution or in other words, image upscaling. Here, a course image goes in and the AI guesses what this image could be and synthesizes a new really detailed image. Now, hold onto your papers and look at that. The difference can be really huge up to a thousand times more pixels in the new image. So, how does it compare to previous methods? For instance, the amazing stable diffusion. Oh my, look at that. It is so much better across the board, pretty much everywhere. But the eyes, the eyes are truly something else with the new technique. What a time to be alive! And finally, fourth. It also offers a decent tangled', metadata={'chunk_size': 128, 'end': 158.5, 'start': 102.68})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_from_llm.get_relevant_documents(query=\"advice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228aae49-4aad-4c65-8f72-aa346ea2886d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d18d2dc4-ce11-41d3-a7df-5eb906877dd4",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0f5fe5a6-e54b-4311-818d-ff384f7def0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_summary = '''\n",
    "Article: {ARTICLE}\n",
    "You will generate increasingly concise, entity-dense summaries of the above article.\n",
    "\n",
    "Repeat the following 2 steps 5 times.\n",
    "\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary.\n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities.\n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main story,\n",
    "- specific yet concise (5 words or fewer),\n",
    "- novel (not in the previous summary),\n",
    "- faithful (present in the article),\n",
    "- anywhere (can be located anywhere in the article).\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article.\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "'''\n",
    "\n",
    "temple_transcription =  '''\n",
    "Transcription: {TRANSCRIPTION}\n",
    "You will generate increasingly concise, entity-dense summaries of the above webinar or conference transcription.\n",
    "\n",
    "Repeat the following 2 steps 5 times.\n",
    "\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the webinar or conference transcription which are missing from the previously generated summary. These entities could include key quotes, pivotal moments, technical terms, etc.\n",
    "Step 2. Write a new, denser summary of identical length that covers every entity and detail from the previous summary, plus the missing entities.\n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main ideas or experience,\n",
    "- specific yet concise (5 words or fewer),\n",
    "- novel (not in the previous summary),\n",
    "- faithful (present in the webinar or conference transcription),\n",
    "- anywhere (can appear at any point during the transcription).\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- The first summary should be long (4-5 sentences, ~80 words) and may focus on the overarching themes discussed. Use verbose language and fillers (e.g., \"the transcription reveals discussions about\") to reach ~80 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and the removal of uninformative phrases like \"it was mentioned that\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without having to read the full transcription.\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b95145ab-b7e8-45f6-913a-50e9ab86e2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a03b02e-8941-434e-9bdc-2aac7dedffb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "356726ed-4e4f-4432-ac9c-76c843e8707d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f6f1d6f7-4ab6-4d1d-93d3-e652ecb59b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1a607983-184a-4a60-b77f-dbb35c86e4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ea64b50d-51ce-4179-aa34-ec711e3c8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_template = \"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                      Write a concise summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d2df83a5-6f65-4585-9b38-aea32ab02592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6750fcb4-0e9e-4c5d-b318-6df7361c70f1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open('state_of_the_union.txt') as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "190ad99b-daa7-482d-8771-ddb5d93e6260",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # length_function = len,\n",
    "    chunk_size=100, chunk_overlap=50, \n",
    ")\n",
    "split_docs = text_splitter.create_documents([state_of_the_union])\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "92ac8102-1dcf-47bd-9471-55f8c0491f65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 20,\n",
    "    # length_function = len,\n",
    "    # is_separator_regex = False,\n",
    ")\n",
    "texts = text_splitter.create_documents([data[\"text\"]])\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8c3b07a1-2474-4542-80e0-e0b027c710a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"It is my pleasure to welcome Dr. Andrew Wu tonight. Andrew is the managing general partner of AI Fund, founder of Deep Learning AI, and lending AI, Chairman and co-founder of Coursera, and an unjunct professor of computer science here at Stanford. Previously he had started and led the Google Brain Team, which had helped Google adopt modern AI, and he was also director of the Stanford AI lab. From 8 million people, one in 1,000 persons on the planet have taken an AI class from him, and through both his education and his AI work, he has changed humor's lives. Please welcome Dr. Andrew Wu. Thank you Lisa, it's good to see everyone. So what I want to do today is chat to you about some opportunities in AI. So I've been saying AI is a new electricity. One of the difficult things to understand about AI is that it is a general purpose technology, meaning that it's not useful only for one thing, but it's useful for lots of different applications, kind of like electricity. If I was to ask you\", metadata={}),\n",
       "  Document(page_content=\"If I was to ask you what is electricity good for, you know, it's not any one thing, there's a lot of things. So what I'd like to do is start off sharing with you, how I view the technology landscape, and just to lead into the set of opportunities. So a lot of excitement about AI. And I think a good way to think about AI is as a collection of tools. So this includes a technique called supervised learning, which is very good at recognizing things or labeling things, and genitive AI, which is relatively new, exciting development. If you're familiar with AI, you may have heard of other tools, but I'm going to talk less about these additional tools. And I'll focus today on what I think are currently the two most important tools, which are supervised learning and genitive AI. So supervised learning is very good at labeling things or very good at computing input to outputs or A to B mappings, given input A, give me an output B. For example, given an email, we can use supervised learning to\", metadata={}),\n",
       "  Document(page_content=\"learning to label a spam or not spam. The most lucrative application of this that I've ever worked on is pervy online advertising where I give it an ad. We can label the users likely to click on it and therefore show more relevant ads. For self-driving cars, given the sense of readings of a car, we can label it with where the other cars. One project that my team AFN worked on was shift route optimization, where given a route to the ship is taking or considering taking, we can label that with how much fuel we think does to consume and use this to make ships more fuel efficient. Still a lot of work in automated visual inspection and factories, so you can take a picture of a smartphone that was just manufactured and label is a scratch when you're defecting it. Or if you want to build a restaurant review reputation monitoring system, you can have little piece of software that looks at online restaurant reviews and labels that as positive or negative sentiment. So one nice thing, one cool\", metadata={}),\n",
       "  Document(page_content=\"thing, one cool thing about supervised learning is that it's not useful for one thing, it's useful for all of these different applications and many more besides. Let me just walk through, concretely, the workflow of one example of a supervised learning labeling things kind of project. If you want to build a system to label restaurant reviews, you then collect a few data points, a collective dataset where it's say, the best time you sandwich great, you say that is positive, several slow, that's negative, my favorite should be curry, that's positive. And here I've shown three data points, but you're building this, you may get thousands of data points like this, thousands of training examples, we call it. And the workflow of a machine learning project, when AI project is you get labeled data, maybe thousands of data points, then you have an AI Android team train an AI model to learn from this data. And then finally, you would find maybe a cloud service to run the trained AI model and\", metadata={}),\n",
       "  Document(page_content=\"AI model and then you can feed it, let's both have a hat and that's positive sentiment. And so I think the last decade was maybe the decade of large-scale supervised learning. What we found starting about 10, 15 years ago was if you were to train a small AI model, so train a small neural network, small deep learning algorithm, basically a small AI model, maybe not on a very powerful computer, then as you feed more data, its performance would get better for a little bit, but then it would flatten out, it would plateau, and it would stop being able to use the data to get better and better. But if you were to train a very large AI model, lots of compute on maybe powerful GPUs, then as we scaled up the amount of data we gave the machine learning model, its performance would kind of keep on getting better and better. So this is why when I started and led the Google Brain team, the primary mission, the AI directed the team to solve at the time was let's just build really, really large\", metadata={}),\n",
       "  Document(page_content=\"really large neural networks that we then fed a lot of data to and that recipe fortunately worked. And I think the idea of driving large compute and large scale data that recipes really helped us driven a lot of AI progress over the last decade. So if that was the last decade of AI, I think this decade is turning out to be also doing everything we had in supervised learning, but adding to it, the exciting two of genitive AI. So many of you, maybe all of you, were played with charge GPD and bar and so on, but just given a piece of text, which we call prompt, I love eating, if you run this multiple times, maybe you get big screen trees or my mother's me low of all out of friends and the AI system can generate output like that. Given the amounts of buzz and excitement about genitive AI, I thought that it'd take just half a slide to say a little bit about how this works. So it turns out that genitive AI, at least this type of text generation, the core of it is using supervised learning\", metadata={}),\n",
       "  Document(page_content='supervised learning that inputs output mappings to repeatedly predict the next word. And so if your system reads on the internet to sentence like, my favorite food is a bagel with cream cheese and locks, then this is translated into a few data points where if it sees my favorite food is a, in this case, try to guess that the right next word was bagel or my favorite food is a bagel, try to guess next word is worth and similarly, if it sees that, in this case, the right guess for the next word would have been cream. So by taking text that you find on the internet or other sources and by using this input output, supervised learning to try to repeatedly predict the next word, if you train a very large AI system on hundreds of billions of words, or in the case of the largest models, now more than a trillion words, then you get a large language model like chat gb. And there are additional other important technical details I talked about predicting the next word. Technically, these systems', metadata={}),\n",
       "  Document(page_content=\"these systems predict the next sub-world, a part of work called token, and then there are other techniques like RHDF for further tuning the AI output to be more helpful on this and harmless. But at the heart of it, is this using supervised learning to repeatedly predict the next word? That's really what's enabling the exciting, you know, really fantastic progress on large language models. So while many people have seen large language models as a fantastic consumer too, you can go to a website like chat gb's website or bots or other large language models and users. I think it's fantastic too. There's one of the trends I think is still underappreciated, which is the power of large language models, not as it, not as it, not just as a consumer too, but as a developer too. So it turns out that there are applications that used to take me months to build that a lot of people can now build much faster by using a large language model. So specifically, the workflow for supervised learning,\", metadata={}),\n",
       "  Document(page_content=\"learning, building the restaurant review system would be that you need to get a bunch of label data and you know, maybe that takes a month to get a few thousand data points. And then have an AI team train and tune and really get, you know, optimized performance on your AI model. Maybe that'll take three months. Then find a cloud service to run it, make sure it's running robustly, make sure it's recognized, maybe that will take another three months. So a pretty realistic timeline for building a commercial grade machine learning system is like six to 12 months. So teams I've led will often tell roughly six to 12 months to build and deploy these systems and some of them turned out to really valuable, but this is a realistic timeline for building and deploying a commercial grade AI system. In contrast, with prompt-based AI, where you write a prompt, this is what the workflow looks like. You can specify a prompt that takes maybe minutes or hours and then you can deploy it to the cloud and\", metadata={}),\n",
       "  Document(page_content=\"it to the cloud and that takes maybe hours or days. So there are now certain AI applications that used to take me, you know, literally six months, maybe a year to build, that many teams around the world can now build in maybe a week. And I think this is already starting, but the best is still yet to come. This is starting to open up a flood of a lot more AI applications that can be built by a lot of people. So I think many people still underestimate the magnitude of the flood of custom AI applications that I think is going to come down the pipe. Now, I know you probably were not expecting me to write code in this presentation, but that's what I'm going to do. So it turns out this is all the code I need in order to write a sentiment classifier. So I'm going to, you know, so you will know Python, I guess, import some tools from OpenAI. And then I have this prompt that says classified detects the low, delimited by three dashes, is having either a positive or negative sentiment. I know\", metadata={}),\n",
       "  Document(page_content=\"sentiment. I know I'm a hard guy. Another fantastic time stand for GSB. We're in a lot and also made great new friends. All right. So that's my prompt. And now just I run it. And I've never run it before. So I really hope, thank goodness we've got the right answer. And this is literally all the code that takes the build a sentiment classifier. And so today, you know, developers around the world can take literally maybe like 10 minutes to build a system like this. And that's a very exciting development. So one of the things I've been working on was trying to teach, you know, online classes about how the youth prompting not just as a consumer to, but as a developer to. So just about the technology landscape. Let me now share my thoughts on what are some of the AI opportunities I see. This shows what I think is the value of different AI technologies today. I don't talk about three years from now. But the vast majority of financial value from AI today is I think supervised learning where\", metadata={}),\n",
       "  Document(page_content=\"learning where for a single company like Google can be worth more than 100 billion US dollars a year. And also there are millions of developers building supervised learning applications. So it's already massively valuable. And also with tremendous momentum behind it, just because of the sheer effort in, you know, finding applications and building applications. And in Gen. of AI is the really exciting new entrance, which is much smaller right now. And then there are the other tools that I'm including for completeness because, you know, if the size of these circles represent the value today, this is one I think I might grow to in three years. So supervised learning already really massive may double say in the next three years from truly massive to even more massive. And Gen. of AI, which is much smaller today, I think we'll much more than double in the next three years because of the number of amount of developer interest, the amount of venture capital investments, the number of large\", metadata={}),\n",
       "  Document(page_content=\"the number of large corporate exploring applications. And I also just want to point out three years is a very short time horizon. If it continues to compound anything near this rate, then it's six years, you know, it'll be even faster larger. But just light shaded region in green or orange, that light shaded region is where the opportunities for either new startups or for large companies in companies to create and to enjoy value capture. But one thing I hope you take away from this slide is that all of these technologies are general purpose technologies. So in the case of supervised learning, a lot of the work that had to be done over the last decade, but it's continuing for the next decade is to identify and to execute on the concrete use cases. And that process is also kicking off for Gen. of AI. So for this part of the presentation, I hope you take away from it that general purpose technologies are useful for many different tasks. A lot of value remains be created using supervised\", metadata={}),\n",
       "  Document(page_content=\"using supervised learning. And even though we're nowhere near finishing figure out the exciting use cases of supervised learning, where there's other, you know, fantastic two of genus of AI, which further expands the set of things we can now do using AI. But one caveat, which is that there will be short term fads along the way. So I don't know if some of you might remember the app called Lenser. This is the app that will let you upload pictures of yourself and then render a cool picture of you as an astronaut or a scientist or something. And it was a good idea and people liked it. And it's obvious just took off like crazy like that through last December. And then it did that. And that's because Lenser was, it was a good idea. People liked it. But it was a relatively thin software layer on top of someone else's really powerful APIs. And so even though it was a useful product, it was in a defensive all business. And when I, when I, when I think about, you know, absolute Lenser, I'm\", metadata={}),\n",
       "  Document(page_content=\"Lenser, I'm actually reminded that when Steve Jobs gave us the iPhone, surely after someone wrote an app that I paid $199.4 to do this, to turn on the LED, to turn the phone into a flashlight. And that was also a good idea to write an app to turn on the LED light. But it also wasn't a defensible long term. It also didn't create very long term value because it was a easy replicated and underpriced and eventually incorporated into iOS. But with the rise of iOS, with the rise of iPhone, someone also figured out how to build things like Uber and Airbnb and Tinder. The very long term, very defensible businesses that created, you know, sustaining value. And I think with the rise of Gen2VI or the rise of new AI tools, I think what really, what excites me is the opportunity to create those really deep, really hard applications that hopefully can create very long term value. So the first trend I want to share is AI's general purpose technology. And a lot of the words that lies ahead of us is\", metadata={}),\n",
       "  Document(page_content=\"lies ahead of us is to find the very diverse use cases and to build them. The second trend I want to share with you, which relates to why AI isn't more widely adopted yet. It feels like a bunch of us have been talking about AI for like 15 years or something. But if you look at where the value of AI is today, a lot of it is still very concentrated and consumer-solve way internet. Once you go outside, you know, tech or consumer-solve internet, there's some air adoption, but the law views very early. So why is that? It turns out if you were to take all current and potential AI projects and sort them in decreasing order of value, then to the left of this curve, the head of this curve are the multi-billion dollar projects like advertising or web search or for e-commerce, your product recommendations or company Amazon. And it turns out that about 10-15 years ago, you know, various of my friends and I, we figured out a recipe for how to hire say a hundred engineers to write one piece of\", metadata={}),\n",
       "  Document(page_content=\"write one piece of software to serve more relevant ads and apply that one piece of software to billion users and generate massive financial value. So that works. But once you go outside, consume a software internet, how do anyone has a hundred million or a billion users that you can write and apply one piece of software to? So once you go to other industries, as we go from the head of this curve on the left over to the long tail, these are some of the projects I see and I'm excited about. I was working with a piece of maker that was taking pictures of the piece that they were making because they needed to do things like make sure that the cheese is spread evenly. So this is about a five million dollar project, but that recipe of hiring a hundred engineers or dozens of engineers to work on a five million dollar project that doesn't make sense. Or another very example, working with an agricultural company that, well then we figured out that we used cameras to find out how tall is the\", metadata={}),\n",
       "  Document(page_content=\"out how tall is the wheat and wheat is often bent over because of wind or rain or something. And we can chop off the wheat at the right height, then that results in more food for the farmer to sell and is also better for the environment. But this is another five million dollar project that that old recipe of having a large group of high school engineers to work on this one project that doesn't make sense. And similarly, materials grading, cloth grading, sheet metal grading, many project like this. So whereas to the left in the head of this curve, there's a small number of let's say multi-billion dollar projects and we know how to execute those, you know, delivering value. In other industries, I'm seeing a very long tail of tens of thousands of, let's call them five million dollar projects that until now have been very difficult to excuse on because of the high cost of customization. The trend that I think is exciting is that the AI community has been building better tools that let's\", metadata={}),\n",
       "  Document(page_content=\"tools that let's us aggregate these use cases and make it easy for the end user to do the customization. So specifically, I'm seeing a lot of exciting low code and no code tools that enable the user to customize the AI system. What this means is instead of me needing to worry that much about pictures of pizza, we have tools, we can start into C2s that can enable the IT department of the pizza making factory to train the AI system on their own pictures of pizza to realize this five billion dollars worth of value. And by the way, the pictures of pizza, you know, they don't exist on the internet. So Google and Bing don't have access to these pictures. We need tools that can be used by really the pizza factory themselves to build and deploy and maintain their own custom AI system that works on their own pictures of pizza. And broadly, the technology for enabling this, some of it is prompting, text prompting, visual prompting, but really large language models and similar tools like that.\", metadata={}),\n",
       "  Document(page_content=\"tools like that. Or a technology called data-centric AI whereby instead of asking the pizza factory to write a lot of code, you know, which is challenging, we can ask them to provide data which turns out to be more feasible. And I think the second trend is important because I think this is a key part of the recipe for taking the value of AI, which so far still feels very concentrated in the tech world and the consumer's offer into that world and pushing this out to, you know, all industries really to the rest of the economy, which, you know, sometimes it's easy to forget. The rest of the economy is much bigger than the tech world. So, the two trends I shared, AI is a general purpose technology, lots of concrete use cases to be realized as well as local, no code, easy to use tools and enabling AI to be deployed in more industries. How do we go after these opportunities? So, about five years ago, there was a puzzle I want to solve, which is I felt that many valuable AI projects are now\", metadata={}),\n",
       "  Document(page_content=\"AI projects are now possible. I was thinking how do we get them done? And having led AI teams in, you know, Google and I do in big tech companies, I had a hard time figuring out how I could operate a team in a big tech company to go off. There are a very diverse set of opportunities and everything from maritime shipping to education to financial services and healthcare and all and all. It's just very diverse use cases, very diverse. Go to markets, very diverse, really, you know, customer bases and then applications. And I felt that the most efficient way to do this would be if we can start a lot of different companies to pursue these very diverse opportunities. So, that's why I end up starting AI fun, which is a venture studio that builds startups to pursue a diverse set of AI opportunities. And of course, in addition to lots of startups, incumbent companies also have a lot of opportunities to integrate AI into existing businesses. In fact, one pattern I'm seeing for incumbent\", metadata={}),\n",
       "  Document(page_content=\"for incumbent businesses is distribution is often one of the cyclical advantages of incumbent companies that they play their cards right can allow them to integrate AI into their products quite efficiently. But just to be concrete, where are the opportunities? So, I think of this as a, this is what I think of as the AI stack. At the bottom level is the hardware semiconductor layer. Fantastic opportunities there, but very capital intensive, very concentrated. So, these are a lot of resources as well as a few winners. So, some people can and should play there. I personally don't like to play them myself. There's also the infrastructure layer. Also, fantastic opportunities, but very capital intensive, very concentrated. So, I tend not to play them myself either. And then there's a developer tool layer. What I showed you just now was I was actually using OpenAI's API as a developer tool. And then I think the developer tool sector is a hyper competitive. Look at all the startups chasing\", metadata={}),\n",
       "  Document(page_content=\"startups chasing OpenAI right now, but there will be some mega winners. And so, I sometimes play here, but primarily when I think of a meaningful technology advantage, because I think that earns you the right or earns you a better shot at being one of the mega winners. And then lastly, even though a lot of the media attention in the buzz is in the infrastructure and developer tooling layer, it turns out that that layer can be successful only if the application layer is even more successful. And we saw this with the rise of SaaS as well. A lot of the buzz the excitement is on the technology, the tooling layer, which is fine, nothing wrong with that. But the only way for that to be successful is that the application layer is even more successful so that frankly, they can generate enough revenue to pay the infrastructure and the tooling layer. So, actually, let me mention one example. I'm on my right, as I actually just texting the CEO yesterday, but I'm on my right is a complete that we\", metadata={}),\n",
       "  Document(page_content=\"a complete that we built that uses AI for romantic relationship coaching. And just to point out, I'm an AI guy, and I feel like I know nothing really about romance. And if you don't believe me, you can ask my wife, she will confirm that I know nothing about romance. But when we want to build this, we want to get together with the former CEO of Tinder, or Renata Nibble, and with my team's expertise in AI, and her expertise in relationships, she ran Tinder. She knows more about relationships. I think anyone I know, we're able to build something pretty unique using AI for romantic relationship mentoring. And the interesting thing about applications like these is when we look around, how many teams in the world are simultaneously expert in AI and in relationships. At the application layer, I'm seeing a lot of exciting opportunities that seem to the very large market, but where the competition set is very light relative to the magnitude of the opportunity. It's not that there are no\", metadata={}),\n",
       "  Document(page_content=\"that there are no competitors, but it's just much less intense compared to the developer tool or the infrastructure layer. And so because I've spent a lot of time iterating on a process of building startups, what I'm going to do is just very transparently tell you the recipe we've developed for building startups. And so after many years of iteration and improvement, this is how we now build startups. My team's always had access to a lot of different ideas, internally generated ideas from partners. And I want to walk through this with one example or something we did, which is a company bearing AI, which uses AI to make ships more fuel efficient. So this idea came to me when a few years ago, a large Japanese conglomerate called Mitsui, that is a major shareholder in the sort of operator major shipping lines. They came to me and they said, Hey, Andrew, you should build a business to use AI to make ships more fuel efficient. And the specific idea was, think of it as a Google Maps for\", metadata={}),\n",
       "  Document(page_content=\"a Google Maps for ships. We can suggest a ship or tell a ship how to steer so that you still get your destination on time, but using turns out about 10% less fuel. And so what we now do is we spend about a month validating the idea. So double-check, this is idea even technically feasible and in top-of-the-prospective customers to make sure that is marketing. So we spend up to about a month doing that. And if it passes this stage, then we will go and recruit a CEO to work with us on the project. When I was starting, I used to spend a long time working on the project myself before bringing on the CEO, but after iterating, we realized that bringing on the leader at the very beginning to work with us, it reduces a lot of the burden of having to transfer knowledge or having a CEO come in and revaluate whether we discover it. So the process is we've learned much more efficient, which is bringing the leader at the very start. And so in the case of bearing AI, we found a fantastic CEO, Dylan\", metadata={}),\n",
       "  Document(page_content=\"CEO, Dylan Kyle, who's a repeat entrepreneur, won successfully exhibit four. And then we spent three months, six two weeks sprints to work with them to build a prototype, as well as do do deep customer validation. If it survives this stage and we have about a 2-thirds 66% survival rate, we never had the first check in, which then gives the company resources to hire an executive team, build the key team, get the MVP working, minimize the survival product working, and get some real customers. And then after that, hopefully then successfully raises additional external rounds of funding that you can keep on growing and scaling. So I'm really proud of the work that my team was able to do to support Mitsui's idea and Dylan Kyle as CEO. And today, there are hundreds of ships on the high seas right now that are steering themselves differently because of bearing AI. And 10% fuel savings translates to rough water man to maybe $450,000 in savings in fuel per ship per year. And of course, it's\", metadata={}),\n",
       "  Document(page_content=\"And of course, it's also frankly quite a bit better for the environment. And I think this is not a, I think would not have existed if not for Dylan's fantastic work. And then also, you know, Mitsui brain this idea to me. And I like this example because this is another one is like, you know, this is a started idea that just a point out I would never have come up with myself, right? Because, you know, I've been on a boat, but what do I know about maritime shipping? But is the deep subject matter expertise of Mitsui that had to zen site together with Dylan and then my team's expertise in AI that made this possible? And so as I operate an AI, one thing I've learned is my swim lane is AI. And that's it because I don't have time. It's very difficult for me to be expert in maritime shipping and romantic relationships and healthcare and financial services and all and all and all. And so I've learned that if I can just help get an accurate technical validation and then use, you know, AI\", metadata={}),\n",
       "  Document(page_content=\"use, you know, AI resources to make sure the AI tech has been quickly and well. And I think we've always managed to help the companies build a strong technical team than partnering with subject matter experts often results in exciting opportunities. And I want to share with you one other weird aspect of one of the weird lessons I've learned about, you know, building startups, which is I like to engage only when there's a concrete idea. And this runs counter to bother the advice you hear from the design thinking methodology, which often says don't rush to solutioning, right? Explore a lot of alternatives for the solution. Honestly, we tried that. It was very slow. But what we've learned is that at the ideation stage, if someone comes to me and says, Hey, Andrew, you should apply AI to financial services because I'm not a subject matter expert in financial services, it's very slow for me to learn enough about financial services to figure out what to do. I mean, eventually you could get\", metadata={}),\n",
       "  Document(page_content=\"you could get a good outcome, but it's a very labor intensive, very slow, very expensive process. So me to try to learn industry after industry. In contrast, one of my partners wrote his ideas that Tony Cheat, not really seriously, but you know, let's say the concrete ideas by GPT, let's eliminate commercials by automatically buying every product advertised in exchange for not having seen ads. It's not a good idea, but it is a concrete idea. And it turns out concrete ideas can be validated or falsified efficiently. They also give a team a clear direction to execute. And I've learned it in today's world, especially with the excitement and buzz and exposure to the AI of a lot of people, it turns out that there are a lot of subject matter experts in today's world that have deeply thought about a problem for months, sometimes even one or two years, but they've not yet had a bill upon there. And when we get together with them and hear and they share the idea of us, it allows us to work\", metadata={}),\n",
       "  Document(page_content=\"allows us to work with them to very quickly go into validation and building. And I find that this works because there are a lot of people that have already done the, you know, design thinking thing of exploring a lot of ideas and winning down to really good ideas. And there are, I find that there's so many good ideas sitting out there that no one is working on that finding those good ideas that someone has already had and wants to share of us and wants to build upon the floor. That turns down to a much more efficient engine. So, before I wrap up, we'll go to the question a second. Just a few slides to talk about risks and social impact. So, AI is a very powerful technology to state something you probably guess. My team's and I, we only work on projects that move humanity forward. And, you know, we have multiple times, code projects that we assess to be financially sound based on ethical grounds. It turns out, I've been surprised and sometimes this made at the creativity of people to\", metadata={}),\n",
       "  Document(page_content=\"of people to come up with good ideas, sorry, to come up with really bad ideas that seem profitable, but really should not be built with a few projects on those, on those grounds. And then I think has to acknowledge that AI today does have problems with bias, fairness, accuracy, but also, you know, technology is improving quickly. So, I see that AI systems today less bias than six months ago and more fair than six months ago, which is not to dismiss the importance of these problems. They are problems and we should continue to work on them. But I'm also gratified at the number of AI tools working hard on these issues to make them much better. When I think of the biggest risk of AI, I think that the biggest risk, one of the biggest disruption to jobs, this is a diagram from a paper by our friend at the University of Pennsylvania and some folks at OpenAI, analyzing the exposure of different jobs to AI automation. And it turns out that whereas the previous wave of automation, mainly the\", metadata={}),\n",
       "  Document(page_content=\"mainly the most exposed jobs were often the lower ways jobs, such as when, you know, we put robots into factories. With this current wave of automation, it's actually the higher-wage jobs further the right of this axis that seems to have more of their tasks exposed to AI automation. So, even as we create tremendous value using AI, I feel like as citizens and our corporations and the governments and really our society, I feel a strong obligation to make sure that people, especially people who are lively who are disrupted, are still well taken care of, are still treated well. And then lastly, it feels like every time there's a big wave of progress in AI, you know, there's a big wave of hype about artificial gender intelligence as well. When deep learning started to work really well 10 years ago, there was a lot of hype about AI and now the general AI is working really well. There's another wave of hype about AI. But I think that artificial intelligence, AI didn't do anything human can\", metadata={}),\n",
       "  Document(page_content=\"anything human can do, it's still decades away, you know, maybe 30 to 50 years, maybe even longer. I hope we'll see in our lifetimes. But I don't think there's any time soon. One of the challenges is that the biological path to intelligence, like humans and the digital path to intelligence, you know, AI, they've taken very different paths. And the funny thing about the definition of AGI is, you know, benchmarking this very different digital path to intelligence with really the biological path to intelligence. So I think, you know, Russian gushmoldos are smarter than any of us in certain key dimensions, but much dumber than any of us in other dimensions. And so forcing it to do everything a human can do is like a funny comparison. But I hope we'll get there, maybe hopefully within our lifetimes. And then there's also a lot of, I think, overblown hype about AI creating extinction risk for humanity. Candidly, I don't see it. I just don't see how AI creates any meaningful extinction risk\", metadata={}),\n",
       "  Document(page_content=\"extinction risk for humanity. I think that people worry we can't control AI, but we have lots of AI will be more powerful than any person, but we've lots of experience steering very powerful entities such as corporations or nation states that are far more powerful than any single person and making sure they for the most part benefit humanity. And also technology develops gradually. The so-called hot takeoff scenario, where it's not really working today, and then suddenly one day overnight, it works brilliantly with your super intelligence takes over the world. That's just not realistic. And I think the AI technology would develop slowly like all the technology, you know, and then it gives us plenty of time to make sure that we provide oversight and can manage it to be safe. And lastly, if you look at the real extinction risk of humanity, such as fingers crossed the next pandemic or climate change leading to a massive depopulation of some parts of the planet, or much lower odds, but\", metadata={}),\n",
       "  Document(page_content='lower odds, but maybe someday, an asteroid doing to us what it had done to the dinosaurs. I think we look at the actual real extinction risk humanity. AI having more intelligence, even artificial intelligence in the world, would be a key part of the solution. So I feel like if you want humanity to survive and thrive for the next thousand years, rather than slowing AI down, which some people propose, I would monitor, I would rather make AI go as fast as possible. So with that, just to summarize, this is my last slide. I think that AI, as a general purpose technology, trees a lot of new opportunities for everyone. And a lot of the exciting and important work that lies ahead of us all is to go and build those concrete use cases. And hopefully in the future, hopefully I have opportunities to maybe engage with more of you on those opportunities as well. So that, let me just say thank you all very much.', metadata={})],\n",
       " 'intermediate_steps': ['to think about all the things that electricity is used for, you would probably come up with a long list. Similarly, AI can be applied to a wide range of industries and fields. Dr. Andrew Wu, who has a background in AI and has taught AI classes to millions of people, believes that AI has the potential to change many aspects of our lives. He compares AI to electricity, stating that it is a general purpose technology that can be used in various applications. In his talk, he will discuss some of the opportunities that AI presents.',\n",
       "  'classify it as spam or not spam. This tool is widely used in various applications such as image recognition, speech recognition, and recommendation systems. On the other hand, generative AI is a newer development that focuses on creating new content or generating new data based on patterns learned from existing data. This tool has been used in applications like image generation, text generation, and music composition. The speaker emphasizes that these two tools, supervised learning and generative AI, are currently the most important in the field of AI.',\n",
       "  'The main points of this text are:\\n\\n- The author has experience in labeling data for various applications, including spam detection in online advertising, labeling cars based on sensor readings, optimizing ship routes for fuel efficiency, automated visual inspection in factories, and sentiment analysis for restaurant reviews.\\n- The author highlights the potential benefits of labeling data in each of these applications, such as showing more relevant ads, improving fuel efficiency, detecting defects, and monitoring restaurant reputation.',\n",
       "  'The main point of this text is that supervised learning is a useful tool for various applications, including labeling restaurant reviews. The process involves collecting a dataset of labeled data points, training an AI model using this data, and then running the trained model on a cloud service. The text mentions that thousands of data points are typically used in this process.',\n",
       "  'The main points of this text are:\\n\\n1. The last decade has been focused on large-scale supervised learning in AI.\\n2. Training a small AI model with limited data and computing power results in performance improvement that eventually plateaus.\\n3. Training a large AI model with powerful GPUs and more data leads to continuous improvement in performance.\\n4. The primary mission of the Google Brain team was to build large AI models.',\n",
       "  'The main points of this text are:\\n\\n- The use of large neural networks and a lot of data has driven AI progress over the last decade.\\n- This decade is focused on adding generative AI to the existing supervised learning techniques.\\n- Generative AI can generate text based on a given prompt using techniques like GPT and BART.\\n- The core of generative AI is based on supervised learning.',\n",
       "  'The text discusses the concept of supervised learning in language models. It explains that by inputting output mappings, the system can predict the next word in a sentence. The example given is if the system reads the sentence \"my favorite food is a bagel with cream cheese and locks,\" it would try to predict the next word based on the previous words. By training a large AI system on a vast amount of text, such as hundreds of billions or even trillions of words, a language model like ChatGPT can be created. The text also mentions that there are other technical details involved in predicting the next word.',\n",
       "  'The main points of this text are:\\n\\n- The use of large language models in predicting the next word or sub-world is enabling exciting progress in the field.\\n- Techniques like RHDF are used to further tune the output of AI systems to be more helpful and harmless.\\n- Large language models are not only beneficial for consumers but also for developers, as they can help in building applications faster.\\n- The workflow for supervised learning is mentioned as a key aspect of using large language models.',\n",
       "  'The main point of this text is that building a restaurant review system using traditional machine learning methods can take around six to 12 months. This includes gathering label data, training and tuning the AI model, and setting up a cloud service to run it. However, with prompt-based AI, the workflow is much quicker. You can write a prompt in minutes or hours and deploy it to the cloud.',\n",
       "  'The main points of this text are:\\n\\n- AI applications that used to take months or even years to build can now be built in a matter of weeks.\\n- This development is opening up opportunities for many people to create custom AI applications.\\n- The speaker believes that there will be a flood of AI applications in the future that many people are underestimating.\\n- The speaker plans to demonstrate writing a sentiment classifier using Python and tools from OpenAI.',\n",
       "  'The main points of this text are:\\n\\n- The author is excited about the development of a sentiment classifier and mentions that it can be built in just 10 minutes.\\n- The author has been working on teaching online classes about technology and the use of AI.\\n- The author shares their thoughts on the current value of AI technologies, specifically mentioning supervised learning as the most valuable for financial gain.',\n",
       "  'The main points of this text are:\\n\\n- Supervised learning is already a highly valuable field, with companies like Google potentially earning over 100 billion US dollars a year from it.\\n- There are millions of developers working on supervised learning applications, contributing to its value and momentum.\\n- The emergence of General Artificial Intelligence (Gen. of AI) is an exciting new development, although it is currently smaller in scale compared to supervised learning.\\n- The author predicts that supervised learning will continue to grow in value, potentially doubling in the next three years.\\n- The author also believes that Gen. of AI will experience significant growth in the next three years due to factors such as developer interest and venture capital investments.',\n",
       "  'The main points of this text are:\\n\\n- The number of large corporations exploring applications is increasing.\\n- The rate of growth for these applications is significant and could lead to even faster growth in the future.\\n- The light shaded region in green or orange represents opportunities for new startups or large companies to create and capture value.\\n- All of these technologies mentioned are general purpose technologies.\\n- The work of identifying and executing concrete use cases for these technologies is ongoing.\\n- General purpose technologies have the potential to be useful for many different tasks and can create value.',\n",
       "  'The main points of this text are:\\n\\n- The use of supervised learning in AI is expanding and has exciting potential use cases.\\n- There may be short-term fads in AI, such as the app called Lenser, which allowed users to upload pictures and render themselves as astronauts or scientists.\\n- Lenser was popular but relied on powerful APIs from another company, making it a relatively thin software layer.\\n- The author reflects on Lenser and its limitations.',\n",
       "  \"The main points of this text are:\\n\\n- The speaker is reminded of an app that turned the iPhone into a flashlight, which was a good idea but not defensible in the long term.\\n- The rise of iOS and the iPhone led to the development of more sustainable and valuable businesses like Uber, Airbnb, and Tinder.\\n- The speaker is excited about the potential of new AI tools to create deep and valuable applications.\\n- The first trend the speaker wants to discuss is AI's general purpose technology.\",\n",
       "  'The main points of this text are:\\n\\n1. The author discusses the need to find diverse use cases for AI and build them.\\n2. The author explains why AI adoption is still limited outside of the tech and consumer internet industries.\\n3. The author mentions that the value of AI projects is currently concentrated in areas like advertising, web search, and product recommendations.\\n4. The author suggests that a recipe for hiring engineers to write AI projects was developed around 10-15 years ago.\\n\\nImportant details include the need to expand the use cases of AI, the limited adoption of AI outside of certain industries, and the existence of a recipe for hiring engineers to work on AI projects.',\n",
       "  'In this chunk of text, the author discusses the challenge of scaling software projects outside of the internet industry. They mention the success of writing software for internet users and generating financial value, but question how to apply this model to industries with a smaller user base. The author gives examples of projects they are excited about, such as using software to ensure even cheese spreading in a manufacturing process and using cameras to measure the height of crops in agriculture. They highlight the difficulty of justifying the cost of hiring a large team of engineers for smaller projects.',\n",
       "  'engineers and developers work on these smaller projects more efficiently and cost-effectively. This includes projects such as optimizing wheat harvesting by chopping off the wheat at the right height, resulting in more food for farmers to sell and being better for the environment. The author mentions that there are also other projects like materials grading, cloth grading, and sheet metal grading that could benefit from these improved tools. The author contrasts these smaller projects with larger, multi-billion dollar projects that are easier to execute. The exciting trend is that the AI community is developing tools that make it easier to work on these smaller projects, which were previously difficult due to the high cost of customization.',\n",
       "  \"The main points of this text are:\\n\\n1. There are tools available that allow users to customize AI systems.\\n2. These tools, known as low code and no code tools, make it easy for users to customize AI systems without needing to worry about specific use cases.\\n3. The example given is a pizza making factory's IT department being able to train an AI system on their own pictures of pizza.\\n4. The value of this customization is estimated to be worth five billion dollars.\\n5. The pictures of pizza used for customization are not available on the internet, so specialized tools are needed.\\n6. The technology used for customization includes prompting, such as text and visual prompts, as well as large language models.\",\n",
       "  \"being developed and implemented in the tech world and consumer sector, but there is a need to expand AI's impact to other industries and the rest of the economy. Two trends that can help achieve this are the realization of concrete use cases for AI and the development of user-friendly, no-code tools. The first trend involves identifying specific applications for AI in various industries, such as healthcare, finance, and manufacturing. The second trend focuses on simplifying the process of implementing AI by creating tools that do not require extensive coding knowledge. This can be achieved through data-centric AI, where companies provide data instead of writing complex code. Expanding AI's reach beyond the tech world and consumer sector is crucial as the rest of the economy is much larger. To pursue these opportunities, it is important to focus on developing and implementing AI projects in different industries.\",\n",
       "  'companies is that they are acquiring AI startups to bring AI capabilities into their existing businesses. This allows them to leverage the expertise and technology of these startups without having to build their own AI teams from scratch. Overall, the main point of the text is that there are a wide range of AI opportunities in various industries, and both startups and incumbent companies are finding ways to pursue these opportunities.',\n",
       "  \"the developer tool market. There are many opportunities in this sector, but it is also highly competitive. The author mentions that they personally do not like to invest in the hardware semiconductor and infrastructure layers, as they require a lot of resources and capital. They prefer to focus on the developer tool layer, where they have used OpenAI's API as an example. Overall, the main points are that distribution is an advantage for incumbent companies, the AI stack consists of hardware semiconductor, infrastructure, and developer tool layers, and the developer tool sector is highly competitive.\",\n",
       "  'The main points of this text are:\\n\\n1. The author believes that there will be some successful startups chasing OpenAI.\\n2. The author plays in this space when they have a meaningful technology advantage.\\n3. The success of the infrastructure and developer tooling layer depends on the success of the application layer.\\n4. The author mentions an example of a startup they are in contact with.',\n",
       "  \"The author discusses their plan to build an AI-powered platform for romantic relationship coaching. They acknowledge their lack of expertise in romance but highlight their team's expertise in AI. They express their intention to collaborate with the former CEO of Tinder, Renata Nibble, who has extensive knowledge in relationships. The author believes that the combination of AI and relationship expertise can create a unique and valuable product. They also mention the potential for a large market with limited competition in this field.\",\n",
       "  'startups. The author explains that while there may not be direct competitors in their field, their approach to building startups is less intense compared to developer tools or infrastructure layers. They mention that they have developed a recipe for building startups through years of iteration and improvement. The author gives an example of a company called Bearing AI, which uses AI to make ships more fuel efficient. This idea was suggested to the author by a large Japanese conglomerate called Mitsui, who saw the potential in using AI for this purpose. The author refers to this idea as a \"Google Maps for startups.\"',\n",
       "  'The main points of this text are:\\n\\n- The author is describing a concept similar to Google Maps, but for ships, that can suggest optimal routes to save fuel.\\n- The idea is being validated for feasibility and market potential, which takes about a month.\\n- If the idea passes this stage, a CEO will be recruited to work on the project.\\n- The author used to work on the project themselves before bringing on a CEO, but now they have realized that bringing on a leader from the beginning is more efficient.\\n- The author has found a CEO named Dylan for their project called bearing AI.',\n",
       "  'The main points of this text are:\\n\\n- The CEO, Dylan Kyle, successfully exhibited at an event called \"four\".\\n- The company spent three months working with Dylan Kyle to build a prototype and validate it with customers.\\n- If the prototype survives this stage, the company receives resources to hire an executive team, build the key team, and get the minimum viable product (MVP) working.\\n- The goal is to raise additional external funding to continue growing and scaling the company.\\n- The team is proud of their work supporting Mitsui\\'s idea and Dylan Kyle as CEO.\\n- The company\\'s product, bearing AI, is being used by hundreds of ships to steer themselves differently.\\n- The product can save up to 10% in fuel costs per ship per year, which translates to around $450,000 in savings.',\n",
       "  \"The main points of this text are:\\n\\n1. The use of AI is better for the environment.\\n2. The author credits Dylan for their fantastic work and Mitsui for suggesting the idea.\\n3. The author acknowledges their lack of expertise in maritime shipping but highlights the collaboration between Mitsui's expertise and their team's expertise in AI.\\n4. The author emphasizes the importance of staying within their swim lane of AI and relying on technical validation for accuracy.\",\n",
       "  'The main points of this text are:\\n\\n1. The author believes in using AI resources to ensure that AI technology is implemented quickly and effectively.\\n2. The author has found that partnering with subject matter experts can lead to exciting opportunities in building a strong technical team.\\n3. The author prefers to engage in projects only when there is a concrete idea, which goes against the advice of design thinking methodology.\\n4. The author has learned that at the ideation stage, it is slow for them to learn about a new industry or subject matter in order to come up with a solution.\\n\\nOverall, the author emphasizes the importance of utilizing AI resources, partnering with subject matter experts, and having a clear idea before starting a project. They also highlight the challenges of learning about new industries or subjects in order to develop solutions.',\n",
       "  \"The main points of this text are:\\n\\n- The process of learning about different industries can be labor-intensive, slow, and expensive.\\n- Concrete ideas, even if they are not good, can be validated or falsified efficiently and provide a clear direction for a team to execute.\\n- In today's world, there are many subject matter experts who have spent a significant amount of time thinking about a problem but have not been able to implement their ideas.\\n- Collaborating with these experts and hearing their ideas allows for productive work.\",\n",
       "  \"The main points of this text are:\\n\\n1. The author believes that working with existing good ideas is more efficient than starting from scratch.\\n2. The author's team only works on projects that have a positive impact on humanity.\\n3. AI is a powerful technology.\\n4. The author considers ethical grounds when assessing the financial viability of projects.\",\n",
       "  'The main points of this text are:\\n\\n- AI can be used to generate bad ideas that may seem profitable but should not be pursued.\\n- AI currently has problems with bias, fairness, and accuracy, but technology is improving.\\n- There are many AI tools working to address these issues.\\n- The biggest risk of AI is the disruption of jobs.\\n- A diagram from a paper by researchers at the University of Pennsylvania and OpenAI shows the exposure of different jobs to AI automation.',\n",
       "  'The main points of this text are:\\n\\n1. Lower-wage jobs, such as those in factories, have been most exposed to automation.\\n2. The current wave of automation is affecting higher-wage jobs more.\\n3. There is a need to ensure that people affected by automation are well taken care of.\\n4. There is always a wave of hype surrounding AI progress.\\n5. The speaker believes that AI cannot do anything that humans can.',\n",
       "  'The main points of this text are:\\n\\n1. Achieving human-level artificial general intelligence (AGI) is still decades away, possibly 30 to 50 years or even longer.\\n2. The biological path to intelligence and the digital path to intelligence (AI) have taken different routes.\\n3. AGI is often benchmarked against human intelligence, but it is important to consider the different dimensions of intelligence.\\n4. The author hopes to see AGI in their lifetime but acknowledges that it is not likely to happen soon.\\n5. The author does not believe that AI poses a significant risk of extinction for humanity and sees the hype around this idea as overblown.',\n",
       "  'still possible, nuclear war, AI is actually one of the smaller risks in comparison. So, while there are concerns about the potential power of AI, the author believes that we have experience in managing powerful entities and can ensure that AI benefits humanity. They also argue that the development of AI will be gradual, allowing for oversight and safety measures to be implemented. Additionally, the author suggests that AI is a smaller risk compared to other potential threats to humanity, such as pandemics, climate change, and nuclear war.',\n",
       "  \"The main points of this text are:\\n\\n1. The speaker believes that the real extinction risk to humanity is not from asteroids, but from the development of artificial intelligence (AI).\\n2. The speaker argues that AI, with its increasing intelligence, can be a key part of the solution to ensure humanity's survival and prosperity.\\n3. Instead of slowing down AI, the speaker suggests that it should be allowed to progress as fast as possible.\\n4. The speaker sees AI as a general purpose technology that presents numerous opportunities for everyone.\\n5. The important work ahead is to build concrete use cases for AI.\\n6. The speaker expresses gratitude and hopes for future opportunities to engage with the audience on these opportunities.\"],\n",
       " 'output_text': \"- AI has the potential to impact various industries and fields\\n- Supervised learning and generative AI are currently important tools in AI\\n- Large-scale supervised learning has been the focus of the last decade in AI\\n- The use of large neural networks and data has driven AI progress\\n- AI applications can now be built in a matter of weeks instead of months or years\\n- The number of large corporations exploring AI applications is increasing\\n- There is a need to expand the use cases of AI and develop user-friendly tools\\n- Customization in AI is estimated to be worth billions of dollars\\n- Opportunities for AI exist in various industries for both startups and incumbent companies\\n- Collaboration between AI experts and subject matter experts is important\\n- AI has challenges with bias, fairness, and accuracy, but technology is improving\\n- Automation affects both lower-wage and higher-wage jobs\\n- Support is needed for those affected by automation\\n- Achieving human-level artificial general intelligence (AGI) is still decades away\\n- AI development will be gradual with oversight and safety measures\\n- AI is a smaller risk compared to other potential threats to humanity\\n- AI can contribute to humanity's survival and prosperity\\n- Building concrete use cases for AI is important.\"}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_outputs = map_reduce_chain({\"input_documents\": texts})\n",
    "map_reduce_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e9000932-4247-4d3a-931b-a2ece5e36096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_documents', 'intermediate_steps', 'output_text'])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0f6512e4-93d7-4871-8051-30dd8764d415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- AI has the potential to impact various industries and fields\n",
      "- Supervised learning and generative AI are currently important tools in AI\n",
      "- Large-scale supervised learning has been the focus of the last decade in AI\n",
      "- The use of large neural networks and data has driven AI progress\n",
      "- AI applications can now be built in a matter of weeks instead of months or years\n",
      "- The number of large corporations exploring AI applications is increasing\n",
      "- There is a need to expand the use cases of AI and develop user-friendly tools\n",
      "- Customization in AI is estimated to be worth billions of dollars\n",
      "- Opportunities for AI exist in various industries for both startups and incumbent companies\n",
      "- Collaboration between AI experts and subject matter experts is important\n",
      "- AI has challenges with bias, fairness, and accuracy, but technology is improving\n",
      "- Automation affects both lower-wage and higher-wage jobs\n",
      "- Support is needed for those affected by automation\n",
      "- Achieving human-level artificial general intelligence (AGI) is still decades away\n",
      "- AI development will be gradual with oversight and safety measures\n",
      "- AI is a smaller risk compared to other potential threats to humanity\n",
      "- AI can contribute to humanity's survival and prosperity\n",
      "- Building concrete use cases for AI is important.\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_outputs[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cfbb9455-2c10-43af-8a1f-b9433f984f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to think about all the things that electricity is used for, you would probably come up with a long list. Similarly, AI can be applied to a wide range of industries and fields. Dr. Andrew Wu, who has a background in AI and has taught AI classes to millions of people, believes that AI has the potential to change many aspects of our lives. He compares AI to electricity, stating that it is a general purpose technology that can be used in various applications. In his talk, he will discuss some of the opportunities that AI presents.', 'classify it as spam or not spam. This tool is widely used in various applications such as image recognition, speech recognition, and recommendation systems. On the other hand, generative AI is a newer development that focuses on creating new content or generating new data based on patterns learned from existing data. This tool has been used in applications like image generation, text generation, and music composition. The speaker emphasizes that these two tools, supervised learning and generative AI, are currently the most important in the field of AI.', 'The main points of this text are:\\n\\n- The author has experience in labeling data for various applications, including spam detection in online advertising, labeling cars based on sensor readings, optimizing ship routes for fuel efficiency, automated visual inspection in factories, and sentiment analysis for restaurant reviews.\\n- The author highlights the potential benefits of labeling data in each of these applications, such as showing more relevant ads, improving fuel efficiency, detecting defects, and monitoring restaurant reputation.', 'The main point of this text is that supervised learning is a useful tool for various applications, including labeling restaurant reviews. The process involves collecting a dataset of labeled data points, training an AI model using this data, and then running the trained model on a cloud service. The text mentions that thousands of data points are typically used in this process.', 'The main points of this text are:\\n\\n1. The last decade has been focused on large-scale supervised learning in AI.\\n2. Training a small AI model with limited data and computing power results in performance improvement that eventually plateaus.\\n3. Training a large AI model with powerful GPUs and more data leads to continuous improvement in performance.\\n4. The primary mission of the Google Brain team was to build large AI models.', 'The main points of this text are:\\n\\n- The use of large neural networks and a lot of data has driven AI progress over the last decade.\\n- This decade is focused on adding generative AI to the existing supervised learning techniques.\\n- Generative AI can generate text based on a given prompt using techniques like GPT and BART.\\n- The core of generative AI is based on supervised learning.', 'The text discusses the concept of supervised learning in language models. It explains that by inputting output mappings, the system can predict the next word in a sentence. The example given is if the system reads the sentence \"my favorite food is a bagel with cream cheese and locks,\" it would try to predict the next word based on the previous words. By training a large AI system on a vast amount of text, such as hundreds of billions or even trillions of words, a language model like ChatGPT can be created. The text also mentions that there are other technical details involved in predicting the next word.', 'The main points of this text are:\\n\\n- The use of large language models in predicting the next word or sub-world is enabling exciting progress in the field.\\n- Techniques like RHDF are used to further tune the output of AI systems to be more helpful and harmless.\\n- Large language models are not only beneficial for consumers but also for developers, as they can help in building applications faster.\\n- The workflow for supervised learning is mentioned as a key aspect of using large language models.', 'The main point of this text is that building a restaurant review system using traditional machine learning methods can take around six to 12 months. This includes gathering label data, training and tuning the AI model, and setting up a cloud service to run it. However, with prompt-based AI, the workflow is much quicker. You can write a prompt in minutes or hours and deploy it to the cloud.', 'The main points of this text are:\\n\\n- AI applications that used to take months or even years to build can now be built in a matter of weeks.\\n- This development is opening up opportunities for many people to create custom AI applications.\\n- The speaker believes that there will be a flood of AI applications in the future that many people are underestimating.\\n- The speaker plans to demonstrate writing a sentiment classifier using Python and tools from OpenAI.', 'The main points of this text are:\\n\\n- The author is excited about the development of a sentiment classifier and mentions that it can be built in just 10 minutes.\\n- The author has been working on teaching online classes about technology and the use of AI.\\n- The author shares their thoughts on the current value of AI technologies, specifically mentioning supervised learning as the most valuable for financial gain.', 'The main points of this text are:\\n\\n- Supervised learning is already a highly valuable field, with companies like Google potentially earning over 100 billion US dollars a year from it.\\n- There are millions of developers working on supervised learning applications, contributing to its value and momentum.\\n- The emergence of General Artificial Intelligence (Gen. of AI) is an exciting new development, although it is currently smaller in scale compared to supervised learning.\\n- The author predicts that supervised learning will continue to grow in value, potentially doubling in the next three years.\\n- The author also believes that Gen. of AI will experience significant growth in the next three years due to factors such as developer interest and venture capital investments.', 'The main points of this text are:\\n\\n- The number of large corporations exploring applications is increasing.\\n- The rate of growth for these applications is significant and could lead to even faster growth in the future.\\n- The light shaded region in green or orange represents opportunities for new startups or large companies to create and capture value.\\n- All of these technologies mentioned are general purpose technologies.\\n- The work of identifying and executing concrete use cases for these technologies is ongoing.\\n- General purpose technologies have the potential to be useful for many different tasks and can create value.', 'The main points of this text are:\\n\\n- The use of supervised learning in AI is expanding and has exciting potential use cases.\\n- There may be short-term fads in AI, such as the app called Lenser, which allowed users to upload pictures and render themselves as astronauts or scientists.\\n- Lenser was popular but relied on powerful APIs from another company, making it a relatively thin software layer.\\n- The author reflects on Lenser and its limitations.', \"The main points of this text are:\\n\\n- The speaker is reminded of an app that turned the iPhone into a flashlight, which was a good idea but not defensible in the long term.\\n- The rise of iOS and the iPhone led to the development of more sustainable and valuable businesses like Uber, Airbnb, and Tinder.\\n- The speaker is excited about the potential of new AI tools to create deep and valuable applications.\\n- The first trend the speaker wants to discuss is AI's general purpose technology.\", 'The main points of this text are:\\n\\n1. The author discusses the need to find diverse use cases for AI and build them.\\n2. The author explains why AI adoption is still limited outside of the tech and consumer internet industries.\\n3. The author mentions that the value of AI projects is currently concentrated in areas like advertising, web search, and product recommendations.\\n4. The author suggests that a recipe for hiring engineers to write AI projects was developed around 10-15 years ago.\\n\\nImportant details include the need to expand the use cases of AI, the limited adoption of AI outside of certain industries, and the existence of a recipe for hiring engineers to work on AI projects.', 'In this chunk of text, the author discusses the challenge of scaling software projects outside of the internet industry. They mention the success of writing software for internet users and generating financial value, but question how to apply this model to industries with a smaller user base. The author gives examples of projects they are excited about, such as using software to ensure even cheese spreading in a manufacturing process and using cameras to measure the height of crops in agriculture. They highlight the difficulty of justifying the cost of hiring a large team of engineers for smaller projects.', 'engineers and developers work on these smaller projects more efficiently and cost-effectively. This includes projects such as optimizing wheat harvesting by chopping off the wheat at the right height, resulting in more food for farmers to sell and being better for the environment. The author mentions that there are also other projects like materials grading, cloth grading, and sheet metal grading that could benefit from these improved tools. The author contrasts these smaller projects with larger, multi-billion dollar projects that are easier to execute. The exciting trend is that the AI community is developing tools that make it easier to work on these smaller projects, which were previously difficult due to the high cost of customization.', \"The main points of this text are:\\n\\n1. There are tools available that allow users to customize AI systems.\\n2. These tools, known as low code and no code tools, make it easy for users to customize AI systems without needing to worry about specific use cases.\\n3. The example given is a pizza making factory's IT department being able to train an AI system on their own pictures of pizza.\\n4. The value of this customization is estimated to be worth five billion dollars.\\n5. The pictures of pizza used for customization are not available on the internet, so specialized tools are needed.\\n6. The technology used for customization includes prompting, such as text and visual prompts, as well as large language models.\", \"being developed and implemented in the tech world and consumer sector, but there is a need to expand AI's impact to other industries and the rest of the economy. Two trends that can help achieve this are the realization of concrete use cases for AI and the development of user-friendly, no-code tools. The first trend involves identifying specific applications for AI in various industries, such as healthcare, finance, and manufacturing. The second trend focuses on simplifying the process of implementing AI by creating tools that do not require extensive coding knowledge. This can be achieved through data-centric AI, where companies provide data instead of writing complex code. Expanding AI's reach beyond the tech world and consumer sector is crucial as the rest of the economy is much larger. To pursue these opportunities, it is important to focus on developing and implementing AI projects in different industries.\", 'companies is that they are acquiring AI startups to bring AI capabilities into their existing businesses. This allows them to leverage the expertise and technology of these startups without having to build their own AI teams from scratch. Overall, the main point of the text is that there are a wide range of AI opportunities in various industries, and both startups and incumbent companies are finding ways to pursue these opportunities.', \"the developer tool market. There are many opportunities in this sector, but it is also highly competitive. The author mentions that they personally do not like to invest in the hardware semiconductor and infrastructure layers, as they require a lot of resources and capital. They prefer to focus on the developer tool layer, where they have used OpenAI's API as an example. Overall, the main points are that distribution is an advantage for incumbent companies, the AI stack consists of hardware semiconductor, infrastructure, and developer tool layers, and the developer tool sector is highly competitive.\", 'The main points of this text are:\\n\\n1. The author believes that there will be some successful startups chasing OpenAI.\\n2. The author plays in this space when they have a meaningful technology advantage.\\n3. The success of the infrastructure and developer tooling layer depends on the success of the application layer.\\n4. The author mentions an example of a startup they are in contact with.', \"The author discusses their plan to build an AI-powered platform for romantic relationship coaching. They acknowledge their lack of expertise in romance but highlight their team's expertise in AI. They express their intention to collaborate with the former CEO of Tinder, Renata Nibble, who has extensive knowledge in relationships. The author believes that the combination of AI and relationship expertise can create a unique and valuable product. They also mention the potential for a large market with limited competition in this field.\", 'startups. The author explains that while there may not be direct competitors in their field, their approach to building startups is less intense compared to developer tools or infrastructure layers. They mention that they have developed a recipe for building startups through years of iteration and improvement. The author gives an example of a company called Bearing AI, which uses AI to make ships more fuel efficient. This idea was suggested to the author by a large Japanese conglomerate called Mitsui, who saw the potential in using AI for this purpose. The author refers to this idea as a \"Google Maps for startups.\"', 'The main points of this text are:\\n\\n- The author is describing a concept similar to Google Maps, but for ships, that can suggest optimal routes to save fuel.\\n- The idea is being validated for feasibility and market potential, which takes about a month.\\n- If the idea passes this stage, a CEO will be recruited to work on the project.\\n- The author used to work on the project themselves before bringing on a CEO, but now they have realized that bringing on a leader from the beginning is more efficient.\\n- The author has found a CEO named Dylan for their project called bearing AI.', 'The main points of this text are:\\n\\n- The CEO, Dylan Kyle, successfully exhibited at an event called \"four\".\\n- The company spent three months working with Dylan Kyle to build a prototype and validate it with customers.\\n- If the prototype survives this stage, the company receives resources to hire an executive team, build the key team, and get the minimum viable product (MVP) working.\\n- The goal is to raise additional external funding to continue growing and scaling the company.\\n- The team is proud of their work supporting Mitsui\\'s idea and Dylan Kyle as CEO.\\n- The company\\'s product, bearing AI, is being used by hundreds of ships to steer themselves differently.\\n- The product can save up to 10% in fuel costs per ship per year, which translates to around $450,000 in savings.', \"The main points of this text are:\\n\\n1. The use of AI is better for the environment.\\n2. The author credits Dylan for their fantastic work and Mitsui for suggesting the idea.\\n3. The author acknowledges their lack of expertise in maritime shipping but highlights the collaboration between Mitsui's expertise and their team's expertise in AI.\\n4. The author emphasizes the importance of staying within their swim lane of AI and relying on technical validation for accuracy.\", 'The main points of this text are:\\n\\n1. The author believes in using AI resources to ensure that AI technology is implemented quickly and effectively.\\n2. The author has found that partnering with subject matter experts can lead to exciting opportunities in building a strong technical team.\\n3. The author prefers to engage in projects only when there is a concrete idea, which goes against the advice of design thinking methodology.\\n4. The author has learned that at the ideation stage, it is slow for them to learn about a new industry or subject matter in order to come up with a solution.\\n\\nOverall, the author emphasizes the importance of utilizing AI resources, partnering with subject matter experts, and having a clear idea before starting a project. They also highlight the challenges of learning about new industries or subjects in order to develop solutions.', \"The main points of this text are:\\n\\n- The process of learning about different industries can be labor-intensive, slow, and expensive.\\n- Concrete ideas, even if they are not good, can be validated or falsified efficiently and provide a clear direction for a team to execute.\\n- In today's world, there are many subject matter experts who have spent a significant amount of time thinking about a problem but have not been able to implement their ideas.\\n- Collaborating with these experts and hearing their ideas allows for productive work.\", \"The main points of this text are:\\n\\n1. The author believes that working with existing good ideas is more efficient than starting from scratch.\\n2. The author's team only works on projects that have a positive impact on humanity.\\n3. AI is a powerful technology.\\n4. The author considers ethical grounds when assessing the financial viability of projects.\", 'The main points of this text are:\\n\\n- AI can be used to generate bad ideas that may seem profitable but should not be pursued.\\n- AI currently has problems with bias, fairness, and accuracy, but technology is improving.\\n- There are many AI tools working to address these issues.\\n- The biggest risk of AI is the disruption of jobs.\\n- A diagram from a paper by researchers at the University of Pennsylvania and OpenAI shows the exposure of different jobs to AI automation.', 'The main points of this text are:\\n\\n1. Lower-wage jobs, such as those in factories, have been most exposed to automation.\\n2. The current wave of automation is affecting higher-wage jobs more.\\n3. There is a need to ensure that people affected by automation are well taken care of.\\n4. There is always a wave of hype surrounding AI progress.\\n5. The speaker believes that AI cannot do anything that humans can.', 'The main points of this text are:\\n\\n1. Achieving human-level artificial general intelligence (AGI) is still decades away, possibly 30 to 50 years or even longer.\\n2. The biological path to intelligence and the digital path to intelligence (AI) have taken different routes.\\n3. AGI is often benchmarked against human intelligence, but it is important to consider the different dimensions of intelligence.\\n4. The author hopes to see AGI in their lifetime but acknowledges that it is not likely to happen soon.\\n5. The author does not believe that AI poses a significant risk of extinction for humanity and sees the hype around this idea as overblown.', 'still possible, nuclear war, AI is actually one of the smaller risks in comparison. So, while there are concerns about the potential power of AI, the author believes that we have experience in managing powerful entities and can ensure that AI benefits humanity. They also argue that the development of AI will be gradual, allowing for oversight and safety measures to be implemented. Additionally, the author suggests that AI is a smaller risk compared to other potential threats to humanity, such as pandemics, climate change, and nuclear war.', \"The main points of this text are:\\n\\n1. The speaker believes that the real extinction risk to humanity is not from asteroids, but from the development of artificial intelligence (AI).\\n2. The speaker argues that AI, with its increasing intelligence, can be a key part of the solution to ensure humanity's survival and prosperity.\\n3. Instead of slowing down AI, the speaker suggests that it should be allowed to progress as fast as possible.\\n4. The speaker sees AI as a general purpose technology that presents numerous opportunities for everyone.\\n5. The important work ahead is to build concrete use cases for AI.\\n6. The speaker expresses gratitude and hopes for future opportunities to engage with the audience on these opportunities.\"]\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_outputs[\"intermediate_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf789d4-eeb1-4069-ae79-7e6fa37ba34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer",
   "language": "python",
   "name": "summarizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
